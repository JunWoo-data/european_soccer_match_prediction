{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost / gbm / light gmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import missingno as msno\n",
    "\n",
    "import sqlite3 as sql \n",
    "\n",
    "import pingouin as pg\n",
    "from random import sample\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have 4 kinds of variable sets \n",
    "    - variable set 1: player attributes PC features\n",
    "    - Variable set 2: betting information features\n",
    "    - variable set 3: team attributes features\n",
    "    - variable set 4: goal and win percentage rolling features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By using each variable sets, let's predict the match result and compare the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_player_attr_pcs = pd.read_csv(\"../data/df_match_player_attr_pcs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_betting_stat = pd.read_csv(\"../data/df_match_betting_stat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_team_num_attr = pd.read_csv(\"../data/df_match_team_num_attr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_team_win_goal_rolling_features = pd.read_csv(\"../data/df_team_win_goal_rolling_features.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Train test split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set last season as test set, other seasons as train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_basic = pd.read_csv(\"../data/df_match_basic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bool = (df_match_basic.match_api_id.isin(df_match_player_attr_pcs.match_api_id)) & \\\n",
    "              (df_match_basic.match_api_id.isin(df_match_betting_stat.match_api_id)) & \\\n",
    "              (df_match_basic.match_api_id.isin(df_match_team_num_attr.match_api_id)) & \\\n",
    "              (df_match_basic.match_api_id.isin(df_team_win_goal_rolling_features.match_api_id)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matches = df_match_basic[target_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_match_api_id = target_matches[target_matches.season == \"2015/2016\"].match_api_id\n",
    "train_match_api_id = target_matches[target_matches.season != \"2015/2016\"].match_api_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17029 2662\n"
     ]
    }
   ],
   "source": [
    "print(len(train_match_api_id), len(test_match_api_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Variable set 1: Player attributes PC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_player_attr_pcs = df_match_player_attr_pcs.merge(df_match_basic[[\"match_api_id\", \"match_result\"]], how = \"left\", on = \"match_api_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_player_1_pc_1</th>\n",
       "      <th>home_player_1_pc_2</th>\n",
       "      <th>home_player_1_pc_3</th>\n",
       "      <th>home_player_1_pc_4</th>\n",
       "      <th>home_player_1_pc_5</th>\n",
       "      <th>home_player_2_pc_1</th>\n",
       "      <th>home_player_2_pc_2</th>\n",
       "      <th>home_player_2_pc_3</th>\n",
       "      <th>home_player_2_pc_4</th>\n",
       "      <th>home_player_2_pc_5</th>\n",
       "      <th>home_player_3_pc_1</th>\n",
       "      <th>home_player_3_pc_2</th>\n",
       "      <th>home_player_3_pc_3</th>\n",
       "      <th>home_player_3_pc_4</th>\n",
       "      <th>home_player_3_pc_5</th>\n",
       "      <th>home_player_4_pc_1</th>\n",
       "      <th>home_player_4_pc_2</th>\n",
       "      <th>home_player_4_pc_3</th>\n",
       "      <th>home_player_4_pc_4</th>\n",
       "      <th>home_player_4_pc_5</th>\n",
       "      <th>home_player_5_pc_1</th>\n",
       "      <th>home_player_5_pc_2</th>\n",
       "      <th>home_player_5_pc_3</th>\n",
       "      <th>home_player_5_pc_4</th>\n",
       "      <th>home_player_5_pc_5</th>\n",
       "      <th>home_player_6_pc_1</th>\n",
       "      <th>home_player_6_pc_2</th>\n",
       "      <th>home_player_6_pc_3</th>\n",
       "      <th>home_player_6_pc_4</th>\n",
       "      <th>home_player_6_pc_5</th>\n",
       "      <th>home_player_7_pc_1</th>\n",
       "      <th>home_player_7_pc_2</th>\n",
       "      <th>home_player_7_pc_3</th>\n",
       "      <th>home_player_7_pc_4</th>\n",
       "      <th>home_player_7_pc_5</th>\n",
       "      <th>home_player_8_pc_1</th>\n",
       "      <th>home_player_8_pc_2</th>\n",
       "      <th>home_player_8_pc_3</th>\n",
       "      <th>home_player_8_pc_4</th>\n",
       "      <th>home_player_8_pc_5</th>\n",
       "      <th>home_player_9_pc_1</th>\n",
       "      <th>home_player_9_pc_2</th>\n",
       "      <th>home_player_9_pc_3</th>\n",
       "      <th>home_player_9_pc_4</th>\n",
       "      <th>home_player_9_pc_5</th>\n",
       "      <th>home_player_10_pc_1</th>\n",
       "      <th>home_player_10_pc_2</th>\n",
       "      <th>home_player_10_pc_3</th>\n",
       "      <th>home_player_10_pc_4</th>\n",
       "      <th>home_player_10_pc_5</th>\n",
       "      <th>home_player_11_pc_1</th>\n",
       "      <th>home_player_11_pc_2</th>\n",
       "      <th>home_player_11_pc_3</th>\n",
       "      <th>home_player_11_pc_4</th>\n",
       "      <th>home_player_11_pc_5</th>\n",
       "      <th>away_player_1_pc_1</th>\n",
       "      <th>away_player_1_pc_2</th>\n",
       "      <th>away_player_1_pc_3</th>\n",
       "      <th>away_player_1_pc_4</th>\n",
       "      <th>away_player_1_pc_5</th>\n",
       "      <th>away_player_2_pc_1</th>\n",
       "      <th>away_player_2_pc_2</th>\n",
       "      <th>away_player_2_pc_3</th>\n",
       "      <th>away_player_2_pc_4</th>\n",
       "      <th>away_player_2_pc_5</th>\n",
       "      <th>away_player_3_pc_1</th>\n",
       "      <th>away_player_3_pc_2</th>\n",
       "      <th>away_player_3_pc_3</th>\n",
       "      <th>away_player_3_pc_4</th>\n",
       "      <th>away_player_3_pc_5</th>\n",
       "      <th>away_player_4_pc_1</th>\n",
       "      <th>away_player_4_pc_2</th>\n",
       "      <th>away_player_4_pc_3</th>\n",
       "      <th>away_player_4_pc_4</th>\n",
       "      <th>away_player_4_pc_5</th>\n",
       "      <th>away_player_5_pc_1</th>\n",
       "      <th>away_player_5_pc_2</th>\n",
       "      <th>away_player_5_pc_3</th>\n",
       "      <th>away_player_5_pc_4</th>\n",
       "      <th>away_player_5_pc_5</th>\n",
       "      <th>away_player_6_pc_1</th>\n",
       "      <th>away_player_6_pc_2</th>\n",
       "      <th>away_player_6_pc_3</th>\n",
       "      <th>away_player_6_pc_4</th>\n",
       "      <th>away_player_6_pc_5</th>\n",
       "      <th>away_player_7_pc_1</th>\n",
       "      <th>away_player_7_pc_2</th>\n",
       "      <th>away_player_7_pc_3</th>\n",
       "      <th>away_player_7_pc_4</th>\n",
       "      <th>away_player_7_pc_5</th>\n",
       "      <th>away_player_8_pc_1</th>\n",
       "      <th>away_player_8_pc_2</th>\n",
       "      <th>away_player_8_pc_3</th>\n",
       "      <th>away_player_8_pc_4</th>\n",
       "      <th>away_player_8_pc_5</th>\n",
       "      <th>away_player_9_pc_1</th>\n",
       "      <th>away_player_9_pc_2</th>\n",
       "      <th>away_player_9_pc_3</th>\n",
       "      <th>away_player_9_pc_4</th>\n",
       "      <th>away_player_9_pc_5</th>\n",
       "      <th>away_player_10_pc_1</th>\n",
       "      <th>away_player_10_pc_2</th>\n",
       "      <th>away_player_10_pc_3</th>\n",
       "      <th>away_player_10_pc_4</th>\n",
       "      <th>away_player_10_pc_5</th>\n",
       "      <th>away_player_11_pc_1</th>\n",
       "      <th>away_player_11_pc_2</th>\n",
       "      <th>away_player_11_pc_3</th>\n",
       "      <th>away_player_11_pc_4</th>\n",
       "      <th>away_player_11_pc_5</th>\n",
       "      <th>match_result</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_api_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493017</th>\n",
       "      <td>9.172915</td>\n",
       "      <td>-0.705596</td>\n",
       "      <td>1.028500</td>\n",
       "      <td>-0.044401</td>\n",
       "      <td>1.246272</td>\n",
       "      <td>3.957784</td>\n",
       "      <td>1.650964</td>\n",
       "      <td>-2.348632</td>\n",
       "      <td>-0.837480</td>\n",
       "      <td>0.223750</td>\n",
       "      <td>-0.817702</td>\n",
       "      <td>-0.589548</td>\n",
       "      <td>0.195433</td>\n",
       "      <td>-2.144582</td>\n",
       "      <td>1.524434</td>\n",
       "      <td>3.108730</td>\n",
       "      <td>0.633708</td>\n",
       "      <td>-1.772338</td>\n",
       "      <td>0.807727</td>\n",
       "      <td>1.684577</td>\n",
       "      <td>0.615229</td>\n",
       "      <td>-0.611994</td>\n",
       "      <td>-0.845425</td>\n",
       "      <td>0.395221</td>\n",
       "      <td>3.107710</td>\n",
       "      <td>-0.038702</td>\n",
       "      <td>-0.551509</td>\n",
       "      <td>-0.280096</td>\n",
       "      <td>-0.068082</td>\n",
       "      <td>2.645878</td>\n",
       "      <td>1.086047</td>\n",
       "      <td>-2.583583</td>\n",
       "      <td>-0.596348</td>\n",
       "      <td>-1.915882</td>\n",
       "      <td>-0.987251</td>\n",
       "      <td>-0.845848</td>\n",
       "      <td>-0.053597</td>\n",
       "      <td>0.746034</td>\n",
       "      <td>-0.515174</td>\n",
       "      <td>1.763346</td>\n",
       "      <td>4.244623</td>\n",
       "      <td>1.088030</td>\n",
       "      <td>-2.054898</td>\n",
       "      <td>-0.414998</td>\n",
       "      <td>0.270225</td>\n",
       "      <td>-0.559441</td>\n",
       "      <td>1.233335</td>\n",
       "      <td>0.426201</td>\n",
       "      <td>0.609910</td>\n",
       "      <td>1.364324</td>\n",
       "      <td>1.472274</td>\n",
       "      <td>-0.298277</td>\n",
       "      <td>-1.530989</td>\n",
       "      <td>0.743621</td>\n",
       "      <td>-0.875547</td>\n",
       "      <td>9.794795</td>\n",
       "      <td>-0.549117</td>\n",
       "      <td>1.941560</td>\n",
       "      <td>0.281992</td>\n",
       "      <td>0.521328</td>\n",
       "      <td>-1.886782</td>\n",
       "      <td>1.005850</td>\n",
       "      <td>1.291888</td>\n",
       "      <td>0.172012</td>\n",
       "      <td>1.212622</td>\n",
       "      <td>1.320201</td>\n",
       "      <td>1.065244</td>\n",
       "      <td>-0.406996</td>\n",
       "      <td>-1.803778</td>\n",
       "      <td>0.573483</td>\n",
       "      <td>2.628391</td>\n",
       "      <td>0.940615</td>\n",
       "      <td>-0.609534</td>\n",
       "      <td>-1.586848</td>\n",
       "      <td>0.652795</td>\n",
       "      <td>3.207876</td>\n",
       "      <td>0.685128</td>\n",
       "      <td>-1.480756</td>\n",
       "      <td>-1.499414</td>\n",
       "      <td>0.134771</td>\n",
       "      <td>-1.360311</td>\n",
       "      <td>-2.979972</td>\n",
       "      <td>1.181522</td>\n",
       "      <td>-0.726996</td>\n",
       "      <td>1.161218</td>\n",
       "      <td>-1.735000</td>\n",
       "      <td>-0.323721</td>\n",
       "      <td>0.977427</td>\n",
       "      <td>-1.047350</td>\n",
       "      <td>1.307837</td>\n",
       "      <td>-0.004187</td>\n",
       "      <td>1.646099</td>\n",
       "      <td>0.543917</td>\n",
       "      <td>0.101722</td>\n",
       "      <td>0.374692</td>\n",
       "      <td>-1.836650</td>\n",
       "      <td>-2.551881</td>\n",
       "      <td>1.212316</td>\n",
       "      <td>0.139711</td>\n",
       "      <td>1.155972</td>\n",
       "      <td>-0.916797</td>\n",
       "      <td>-1.202214</td>\n",
       "      <td>0.022668</td>\n",
       "      <td>-0.118531</td>\n",
       "      <td>-0.332637</td>\n",
       "      <td>0.628660</td>\n",
       "      <td>-1.751083</td>\n",
       "      <td>-0.030693</td>\n",
       "      <td>0.418133</td>\n",
       "      <td>-0.206630</td>\n",
       "      <td>home_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493025</th>\n",
       "      <td>6.467731</td>\n",
       "      <td>-2.125163</td>\n",
       "      <td>3.092089</td>\n",
       "      <td>-0.930974</td>\n",
       "      <td>1.172527</td>\n",
       "      <td>0.390653</td>\n",
       "      <td>1.341612</td>\n",
       "      <td>0.109198</td>\n",
       "      <td>-0.418330</td>\n",
       "      <td>0.921304</td>\n",
       "      <td>2.673401</td>\n",
       "      <td>1.688787</td>\n",
       "      <td>-0.346764</td>\n",
       "      <td>0.032250</td>\n",
       "      <td>-0.148238</td>\n",
       "      <td>0.544724</td>\n",
       "      <td>0.856618</td>\n",
       "      <td>-0.234054</td>\n",
       "      <td>-0.185530</td>\n",
       "      <td>1.496783</td>\n",
       "      <td>0.349558</td>\n",
       "      <td>-0.322471</td>\n",
       "      <td>0.311433</td>\n",
       "      <td>-0.720916</td>\n",
       "      <td>0.765089</td>\n",
       "      <td>-2.997770</td>\n",
       "      <td>-1.279240</td>\n",
       "      <td>1.492689</td>\n",
       "      <td>-0.174247</td>\n",
       "      <td>1.728035</td>\n",
       "      <td>-1.262416</td>\n",
       "      <td>0.290702</td>\n",
       "      <td>1.097804</td>\n",
       "      <td>0.377514</td>\n",
       "      <td>1.681666</td>\n",
       "      <td>-1.035827</td>\n",
       "      <td>-0.034337</td>\n",
       "      <td>1.133088</td>\n",
       "      <td>0.899750</td>\n",
       "      <td>1.451945</td>\n",
       "      <td>-1.970162</td>\n",
       "      <td>-1.958911</td>\n",
       "      <td>1.072887</td>\n",
       "      <td>0.806278</td>\n",
       "      <td>2.735079</td>\n",
       "      <td>-1.206227</td>\n",
       "      <td>-2.567254</td>\n",
       "      <td>0.401355</td>\n",
       "      <td>-0.240084</td>\n",
       "      <td>1.552955</td>\n",
       "      <td>-0.417405</td>\n",
       "      <td>-0.845298</td>\n",
       "      <td>0.077456</td>\n",
       "      <td>-1.026087</td>\n",
       "      <td>-0.249720</td>\n",
       "      <td>6.746068</td>\n",
       "      <td>-1.225452</td>\n",
       "      <td>5.441467</td>\n",
       "      <td>-1.741143</td>\n",
       "      <td>0.153029</td>\n",
       "      <td>-0.720625</td>\n",
       "      <td>1.362837</td>\n",
       "      <td>0.950063</td>\n",
       "      <td>-0.430052</td>\n",
       "      <td>1.920381</td>\n",
       "      <td>1.321520</td>\n",
       "      <td>2.234319</td>\n",
       "      <td>0.546943</td>\n",
       "      <td>0.392576</td>\n",
       "      <td>1.081937</td>\n",
       "      <td>-1.157135</td>\n",
       "      <td>1.181325</td>\n",
       "      <td>0.901247</td>\n",
       "      <td>0.722505</td>\n",
       "      <td>0.967595</td>\n",
       "      <td>-1.573349</td>\n",
       "      <td>0.366277</td>\n",
       "      <td>1.085719</td>\n",
       "      <td>-0.362088</td>\n",
       "      <td>0.794429</td>\n",
       "      <td>-2.362155</td>\n",
       "      <td>0.960996</td>\n",
       "      <td>2.341796</td>\n",
       "      <td>0.480765</td>\n",
       "      <td>1.235871</td>\n",
       "      <td>-2.036662</td>\n",
       "      <td>1.423269</td>\n",
       "      <td>1.975662</td>\n",
       "      <td>0.456836</td>\n",
       "      <td>0.564672</td>\n",
       "      <td>-1.002934</td>\n",
       "      <td>-2.335553</td>\n",
       "      <td>0.470491</td>\n",
       "      <td>-0.111478</td>\n",
       "      <td>0.585989</td>\n",
       "      <td>-1.407944</td>\n",
       "      <td>-2.205414</td>\n",
       "      <td>1.069296</td>\n",
       "      <td>0.053596</td>\n",
       "      <td>0.809813</td>\n",
       "      <td>-2.259696</td>\n",
       "      <td>-2.778135</td>\n",
       "      <td>1.816538</td>\n",
       "      <td>1.407815</td>\n",
       "      <td>0.022775</td>\n",
       "      <td>0.169010</td>\n",
       "      <td>-2.319129</td>\n",
       "      <td>0.361658</td>\n",
       "      <td>0.935315</td>\n",
       "      <td>-1.877390</td>\n",
       "      <td>away_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493027</th>\n",
       "      <td>7.587977</td>\n",
       "      <td>-0.669761</td>\n",
       "      <td>3.351410</td>\n",
       "      <td>-1.725204</td>\n",
       "      <td>0.971832</td>\n",
       "      <td>-0.659142</td>\n",
       "      <td>2.447278</td>\n",
       "      <td>1.853202</td>\n",
       "      <td>-0.962117</td>\n",
       "      <td>0.277773</td>\n",
       "      <td>0.782575</td>\n",
       "      <td>2.059493</td>\n",
       "      <td>1.311728</td>\n",
       "      <td>-0.983126</td>\n",
       "      <td>0.652947</td>\n",
       "      <td>0.750721</td>\n",
       "      <td>3.078327</td>\n",
       "      <td>1.228950</td>\n",
       "      <td>0.540478</td>\n",
       "      <td>0.034199</td>\n",
       "      <td>0.361849</td>\n",
       "      <td>2.339354</td>\n",
       "      <td>1.689313</td>\n",
       "      <td>0.713260</td>\n",
       "      <td>0.455514</td>\n",
       "      <td>-3.759299</td>\n",
       "      <td>1.204148</td>\n",
       "      <td>3.191002</td>\n",
       "      <td>-0.784951</td>\n",
       "      <td>0.913294</td>\n",
       "      <td>-3.018176</td>\n",
       "      <td>0.545416</td>\n",
       "      <td>2.535436</td>\n",
       "      <td>-0.218828</td>\n",
       "      <td>2.658318</td>\n",
       "      <td>-2.589758</td>\n",
       "      <td>0.570906</td>\n",
       "      <td>1.654008</td>\n",
       "      <td>0.345163</td>\n",
       "      <td>0.580780</td>\n",
       "      <td>-3.859427</td>\n",
       "      <td>-1.870653</td>\n",
       "      <td>2.659663</td>\n",
       "      <td>-0.725774</td>\n",
       "      <td>2.171793</td>\n",
       "      <td>-0.199689</td>\n",
       "      <td>-2.415793</td>\n",
       "      <td>-0.317491</td>\n",
       "      <td>0.067180</td>\n",
       "      <td>0.943378</td>\n",
       "      <td>-1.056848</td>\n",
       "      <td>-1.205950</td>\n",
       "      <td>1.067483</td>\n",
       "      <td>1.246078</td>\n",
       "      <td>-0.070047</td>\n",
       "      <td>8.942737</td>\n",
       "      <td>-2.146976</td>\n",
       "      <td>1.394474</td>\n",
       "      <td>-1.573403</td>\n",
       "      <td>-0.305385</td>\n",
       "      <td>0.326027</td>\n",
       "      <td>1.666843</td>\n",
       "      <td>0.694338</td>\n",
       "      <td>0.105792</td>\n",
       "      <td>1.385551</td>\n",
       "      <td>1.716745</td>\n",
       "      <td>1.133649</td>\n",
       "      <td>-0.670094</td>\n",
       "      <td>-1.374712</td>\n",
       "      <td>0.468374</td>\n",
       "      <td>1.822806</td>\n",
       "      <td>1.126330</td>\n",
       "      <td>-0.789073</td>\n",
       "      <td>-1.478142</td>\n",
       "      <td>0.878564</td>\n",
       "      <td>5.021780</td>\n",
       "      <td>0.645976</td>\n",
       "      <td>-1.954515</td>\n",
       "      <td>-1.642732</td>\n",
       "      <td>0.230055</td>\n",
       "      <td>0.882744</td>\n",
       "      <td>1.703690</td>\n",
       "      <td>0.053693</td>\n",
       "      <td>0.563981</td>\n",
       "      <td>0.978446</td>\n",
       "      <td>-0.780143</td>\n",
       "      <td>-0.700812</td>\n",
       "      <td>0.612057</td>\n",
       "      <td>1.416142</td>\n",
       "      <td>2.059364</td>\n",
       "      <td>-0.691788</td>\n",
       "      <td>-2.016431</td>\n",
       "      <td>0.356132</td>\n",
       "      <td>-1.164456</td>\n",
       "      <td>2.251789</td>\n",
       "      <td>1.464444</td>\n",
       "      <td>0.379332</td>\n",
       "      <td>-0.902796</td>\n",
       "      <td>-0.837762</td>\n",
       "      <td>0.211743</td>\n",
       "      <td>1.923084</td>\n",
       "      <td>-2.893726</td>\n",
       "      <td>-2.042800</td>\n",
       "      <td>-1.018973</td>\n",
       "      <td>-1.292926</td>\n",
       "      <td>0.416905</td>\n",
       "      <td>-2.210474</td>\n",
       "      <td>-0.336256</td>\n",
       "      <td>0.054563</td>\n",
       "      <td>2.588626</td>\n",
       "      <td>home_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493034</th>\n",
       "      <td>9.172915</td>\n",
       "      <td>-0.705596</td>\n",
       "      <td>1.028500</td>\n",
       "      <td>-0.044401</td>\n",
       "      <td>1.246272</td>\n",
       "      <td>3.957784</td>\n",
       "      <td>1.650964</td>\n",
       "      <td>-2.348632</td>\n",
       "      <td>-0.837480</td>\n",
       "      <td>0.223750</td>\n",
       "      <td>-0.817702</td>\n",
       "      <td>-0.589548</td>\n",
       "      <td>0.195433</td>\n",
       "      <td>-2.144582</td>\n",
       "      <td>1.524434</td>\n",
       "      <td>-0.559441</td>\n",
       "      <td>1.233335</td>\n",
       "      <td>0.426201</td>\n",
       "      <td>0.609910</td>\n",
       "      <td>1.364324</td>\n",
       "      <td>-0.845848</td>\n",
       "      <td>-0.053597</td>\n",
       "      <td>0.746034</td>\n",
       "      <td>-0.515174</td>\n",
       "      <td>1.763346</td>\n",
       "      <td>0.615229</td>\n",
       "      <td>-0.611994</td>\n",
       "      <td>-0.845425</td>\n",
       "      <td>0.395221</td>\n",
       "      <td>3.107710</td>\n",
       "      <td>3.108730</td>\n",
       "      <td>0.633708</td>\n",
       "      <td>-1.772338</td>\n",
       "      <td>0.807727</td>\n",
       "      <td>1.684577</td>\n",
       "      <td>1.086047</td>\n",
       "      <td>-2.583583</td>\n",
       "      <td>-0.596348</td>\n",
       "      <td>-1.915882</td>\n",
       "      <td>-0.987251</td>\n",
       "      <td>4.244623</td>\n",
       "      <td>1.088030</td>\n",
       "      <td>-2.054898</td>\n",
       "      <td>-0.414998</td>\n",
       "      <td>0.270225</td>\n",
       "      <td>-0.105750</td>\n",
       "      <td>0.211866</td>\n",
       "      <td>0.619067</td>\n",
       "      <td>0.766076</td>\n",
       "      <td>-0.979514</td>\n",
       "      <td>1.472274</td>\n",
       "      <td>-0.298277</td>\n",
       "      <td>-1.530989</td>\n",
       "      <td>0.743621</td>\n",
       "      <td>-0.875547</td>\n",
       "      <td>7.587977</td>\n",
       "      <td>-0.669761</td>\n",
       "      <td>3.351410</td>\n",
       "      <td>-1.725204</td>\n",
       "      <td>0.971832</td>\n",
       "      <td>-0.659142</td>\n",
       "      <td>2.447278</td>\n",
       "      <td>1.853202</td>\n",
       "      <td>-0.962117</td>\n",
       "      <td>0.277773</td>\n",
       "      <td>0.361849</td>\n",
       "      <td>2.339354</td>\n",
       "      <td>1.689313</td>\n",
       "      <td>0.713260</td>\n",
       "      <td>0.455514</td>\n",
       "      <td>1.052194</td>\n",
       "      <td>2.080899</td>\n",
       "      <td>0.370792</td>\n",
       "      <td>-0.782729</td>\n",
       "      <td>0.394531</td>\n",
       "      <td>2.077104</td>\n",
       "      <td>1.903504</td>\n",
       "      <td>0.684877</td>\n",
       "      <td>1.471026</td>\n",
       "      <td>-0.492695</td>\n",
       "      <td>-3.759299</td>\n",
       "      <td>1.204148</td>\n",
       "      <td>3.191002</td>\n",
       "      <td>-0.784951</td>\n",
       "      <td>0.913294</td>\n",
       "      <td>-0.631336</td>\n",
       "      <td>-1.917288</td>\n",
       "      <td>0.404582</td>\n",
       "      <td>-1.604514</td>\n",
       "      <td>-0.334234</td>\n",
       "      <td>-3.859427</td>\n",
       "      <td>-1.870653</td>\n",
       "      <td>2.659663</td>\n",
       "      <td>-0.725774</td>\n",
       "      <td>2.171793</td>\n",
       "      <td>-3.018176</td>\n",
       "      <td>0.545416</td>\n",
       "      <td>2.535436</td>\n",
       "      <td>-0.218828</td>\n",
       "      <td>2.658318</td>\n",
       "      <td>-2.589758</td>\n",
       "      <td>0.570906</td>\n",
       "      <td>1.654008</td>\n",
       "      <td>0.345163</td>\n",
       "      <td>0.580780</td>\n",
       "      <td>-1.056848</td>\n",
       "      <td>-1.205950</td>\n",
       "      <td>1.067483</td>\n",
       "      <td>1.246078</td>\n",
       "      <td>-0.070047</td>\n",
       "      <td>home_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493040</th>\n",
       "      <td>8.942737</td>\n",
       "      <td>-2.146976</td>\n",
       "      <td>1.394474</td>\n",
       "      <td>-1.573403</td>\n",
       "      <td>-0.305385</td>\n",
       "      <td>0.326027</td>\n",
       "      <td>1.666843</td>\n",
       "      <td>0.694338</td>\n",
       "      <td>0.105792</td>\n",
       "      <td>1.385551</td>\n",
       "      <td>1.831429</td>\n",
       "      <td>1.436093</td>\n",
       "      <td>-0.753915</td>\n",
       "      <td>-1.703966</td>\n",
       "      <td>1.116009</td>\n",
       "      <td>1.716745</td>\n",
       "      <td>1.133649</td>\n",
       "      <td>-0.670094</td>\n",
       "      <td>-1.374712</td>\n",
       "      <td>0.468374</td>\n",
       "      <td>1.822806</td>\n",
       "      <td>1.126330</td>\n",
       "      <td>-0.789073</td>\n",
       "      <td>-1.478142</td>\n",
       "      <td>0.878564</td>\n",
       "      <td>0.882744</td>\n",
       "      <td>1.703690</td>\n",
       "      <td>0.053693</td>\n",
       "      <td>0.563981</td>\n",
       "      <td>0.978446</td>\n",
       "      <td>-0.780143</td>\n",
       "      <td>-0.700812</td>\n",
       "      <td>0.612057</td>\n",
       "      <td>1.416142</td>\n",
       "      <td>2.059364</td>\n",
       "      <td>-0.691788</td>\n",
       "      <td>-2.016431</td>\n",
       "      <td>0.356132</td>\n",
       "      <td>-1.164456</td>\n",
       "      <td>2.251789</td>\n",
       "      <td>0.416905</td>\n",
       "      <td>-2.210474</td>\n",
       "      <td>-0.336256</td>\n",
       "      <td>0.054563</td>\n",
       "      <td>2.588626</td>\n",
       "      <td>1.923084</td>\n",
       "      <td>-2.893726</td>\n",
       "      <td>-2.042800</td>\n",
       "      <td>-1.018973</td>\n",
       "      <td>-1.292926</td>\n",
       "      <td>1.464444</td>\n",
       "      <td>0.379332</td>\n",
       "      <td>-0.902796</td>\n",
       "      <td>-0.837762</td>\n",
       "      <td>0.211743</td>\n",
       "      <td>9.554376</td>\n",
       "      <td>-0.788436</td>\n",
       "      <td>2.277353</td>\n",
       "      <td>0.709577</td>\n",
       "      <td>0.731361</td>\n",
       "      <td>1.970444</td>\n",
       "      <td>1.864556</td>\n",
       "      <td>-0.597320</td>\n",
       "      <td>0.168243</td>\n",
       "      <td>0.246152</td>\n",
       "      <td>4.470705</td>\n",
       "      <td>2.449109</td>\n",
       "      <td>-1.396639</td>\n",
       "      <td>-0.315235</td>\n",
       "      <td>0.008914</td>\n",
       "      <td>1.829497</td>\n",
       "      <td>1.079852</td>\n",
       "      <td>-2.363541</td>\n",
       "      <td>-0.545278</td>\n",
       "      <td>0.775451</td>\n",
       "      <td>4.019384</td>\n",
       "      <td>0.341251</td>\n",
       "      <td>-2.730811</td>\n",
       "      <td>-1.093104</td>\n",
       "      <td>0.247451</td>\n",
       "      <td>0.682845</td>\n",
       "      <td>0.218515</td>\n",
       "      <td>-0.581395</td>\n",
       "      <td>0.951872</td>\n",
       "      <td>2.794600</td>\n",
       "      <td>-0.793495</td>\n",
       "      <td>-0.740710</td>\n",
       "      <td>0.492791</td>\n",
       "      <td>0.151706</td>\n",
       "      <td>2.489086</td>\n",
       "      <td>1.933672</td>\n",
       "      <td>0.891587</td>\n",
       "      <td>-1.109525</td>\n",
       "      <td>-0.792216</td>\n",
       "      <td>-0.402166</td>\n",
       "      <td>-0.203211</td>\n",
       "      <td>-1.625096</td>\n",
       "      <td>-0.009942</td>\n",
       "      <td>0.220891</td>\n",
       "      <td>1.091029</td>\n",
       "      <td>0.344323</td>\n",
       "      <td>-3.281820</td>\n",
       "      <td>-1.047589</td>\n",
       "      <td>-0.870665</td>\n",
       "      <td>0.278254</td>\n",
       "      <td>0.817798</td>\n",
       "      <td>-2.420838</td>\n",
       "      <td>-0.361229</td>\n",
       "      <td>0.689072</td>\n",
       "      <td>-0.850103</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992089</th>\n",
       "      <td>10.658392</td>\n",
       "      <td>-1.977648</td>\n",
       "      <td>0.867377</td>\n",
       "      <td>0.480266</td>\n",
       "      <td>1.074043</td>\n",
       "      <td>2.309500</td>\n",
       "      <td>0.931667</td>\n",
       "      <td>-3.860209</td>\n",
       "      <td>0.540601</td>\n",
       "      <td>-0.235396</td>\n",
       "      <td>4.829365</td>\n",
       "      <td>3.813902</td>\n",
       "      <td>-2.190143</td>\n",
       "      <td>2.896975</td>\n",
       "      <td>0.303158</td>\n",
       "      <td>4.363347</td>\n",
       "      <td>3.346856</td>\n",
       "      <td>-2.511394</td>\n",
       "      <td>1.256390</td>\n",
       "      <td>-1.264653</td>\n",
       "      <td>0.693648</td>\n",
       "      <td>2.017825</td>\n",
       "      <td>-1.753765</td>\n",
       "      <td>-2.152817</td>\n",
       "      <td>-0.662817</td>\n",
       "      <td>2.162406</td>\n",
       "      <td>-2.749882</td>\n",
       "      <td>-4.593596</td>\n",
       "      <td>0.434768</td>\n",
       "      <td>0.841622</td>\n",
       "      <td>-0.817444</td>\n",
       "      <td>1.999270</td>\n",
       "      <td>-0.449227</td>\n",
       "      <td>-0.166485</td>\n",
       "      <td>-0.336412</td>\n",
       "      <td>1.345606</td>\n",
       "      <td>0.069802</td>\n",
       "      <td>-3.170566</td>\n",
       "      <td>-0.790618</td>\n",
       "      <td>1.344050</td>\n",
       "      <td>-1.052756</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>-1.406108</td>\n",
       "      <td>-0.920376</td>\n",
       "      <td>1.149814</td>\n",
       "      <td>-0.756354</td>\n",
       "      <td>-0.555587</td>\n",
       "      <td>-0.498005</td>\n",
       "      <td>0.934758</td>\n",
       "      <td>-2.606863</td>\n",
       "      <td>-2.571317</td>\n",
       "      <td>-2.788252</td>\n",
       "      <td>-0.783057</td>\n",
       "      <td>-0.228549</td>\n",
       "      <td>0.825315</td>\n",
       "      <td>11.297455</td>\n",
       "      <td>-1.943311</td>\n",
       "      <td>-0.622608</td>\n",
       "      <td>1.229927</td>\n",
       "      <td>2.987196</td>\n",
       "      <td>0.715893</td>\n",
       "      <td>1.452546</td>\n",
       "      <td>-2.111059</td>\n",
       "      <td>-0.524585</td>\n",
       "      <td>0.158115</td>\n",
       "      <td>5.742015</td>\n",
       "      <td>3.143138</td>\n",
       "      <td>-3.280495</td>\n",
       "      <td>0.901933</td>\n",
       "      <td>-1.055971</td>\n",
       "      <td>3.559549</td>\n",
       "      <td>2.023672</td>\n",
       "      <td>-2.954817</td>\n",
       "      <td>1.214646</td>\n",
       "      <td>0.395441</td>\n",
       "      <td>-0.652392</td>\n",
       "      <td>0.617508</td>\n",
       "      <td>-2.119288</td>\n",
       "      <td>0.420918</td>\n",
       "      <td>-0.312378</td>\n",
       "      <td>-1.179215</td>\n",
       "      <td>0.990786</td>\n",
       "      <td>-0.758331</td>\n",
       "      <td>0.280326</td>\n",
       "      <td>-1.019536</td>\n",
       "      <td>-0.983237</td>\n",
       "      <td>1.033960</td>\n",
       "      <td>-0.714609</td>\n",
       "      <td>-0.027433</td>\n",
       "      <td>0.645499</td>\n",
       "      <td>-0.468185</td>\n",
       "      <td>-2.492838</td>\n",
       "      <td>-2.135542</td>\n",
       "      <td>0.672037</td>\n",
       "      <td>1.175961</td>\n",
       "      <td>0.089223</td>\n",
       "      <td>-1.409898</td>\n",
       "      <td>-2.184536</td>\n",
       "      <td>0.860498</td>\n",
       "      <td>2.273182</td>\n",
       "      <td>-1.604118</td>\n",
       "      <td>-1.071515</td>\n",
       "      <td>-1.719817</td>\n",
       "      <td>-0.597994</td>\n",
       "      <td>0.268359</td>\n",
       "      <td>-2.027482</td>\n",
       "      <td>-0.929774</td>\n",
       "      <td>-0.429955</td>\n",
       "      <td>-0.408179</td>\n",
       "      <td>-0.159691</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992091</th>\n",
       "      <td>12.241704</td>\n",
       "      <td>-2.064268</td>\n",
       "      <td>0.575912</td>\n",
       "      <td>0.027214</td>\n",
       "      <td>0.566339</td>\n",
       "      <td>2.941632</td>\n",
       "      <td>1.263739</td>\n",
       "      <td>-2.564271</td>\n",
       "      <td>-0.317355</td>\n",
       "      <td>0.251567</td>\n",
       "      <td>-0.338978</td>\n",
       "      <td>1.001774</td>\n",
       "      <td>-1.054876</td>\n",
       "      <td>0.023669</td>\n",
       "      <td>0.503527</td>\n",
       "      <td>3.322061</td>\n",
       "      <td>2.191727</td>\n",
       "      <td>-1.431455</td>\n",
       "      <td>-0.935647</td>\n",
       "      <td>-2.106780</td>\n",
       "      <td>0.805775</td>\n",
       "      <td>2.313243</td>\n",
       "      <td>-1.001739</td>\n",
       "      <td>-0.734564</td>\n",
       "      <td>-1.002397</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.748910</td>\n",
       "      <td>-2.441020</td>\n",
       "      <td>-0.128875</td>\n",
       "      <td>1.354357</td>\n",
       "      <td>-0.992210</td>\n",
       "      <td>1.486864</td>\n",
       "      <td>-0.837892</td>\n",
       "      <td>-0.283414</td>\n",
       "      <td>0.885329</td>\n",
       "      <td>-0.144093</td>\n",
       "      <td>-1.113188</td>\n",
       "      <td>-1.698684</td>\n",
       "      <td>0.131062</td>\n",
       "      <td>0.388273</td>\n",
       "      <td>-1.074581</td>\n",
       "      <td>-2.141164</td>\n",
       "      <td>-1.372831</td>\n",
       "      <td>0.593159</td>\n",
       "      <td>-0.060973</td>\n",
       "      <td>-1.374814</td>\n",
       "      <td>-2.597658</td>\n",
       "      <td>-1.353337</td>\n",
       "      <td>-0.079045</td>\n",
       "      <td>0.449154</td>\n",
       "      <td>1.613120</td>\n",
       "      <td>-2.543332</td>\n",
       "      <td>-2.596794</td>\n",
       "      <td>1.483082</td>\n",
       "      <td>0.105411</td>\n",
       "      <td>12.290704</td>\n",
       "      <td>-2.430347</td>\n",
       "      <td>-1.144252</td>\n",
       "      <td>-0.197618</td>\n",
       "      <td>0.806754</td>\n",
       "      <td>4.561174</td>\n",
       "      <td>1.116819</td>\n",
       "      <td>-3.853639</td>\n",
       "      <td>-1.449020</td>\n",
       "      <td>-0.008049</td>\n",
       "      <td>0.859240</td>\n",
       "      <td>1.593152</td>\n",
       "      <td>-1.523836</td>\n",
       "      <td>0.182434</td>\n",
       "      <td>0.923206</td>\n",
       "      <td>3.366277</td>\n",
       "      <td>2.096648</td>\n",
       "      <td>-2.560049</td>\n",
       "      <td>1.302320</td>\n",
       "      <td>0.621418</td>\n",
       "      <td>-0.179065</td>\n",
       "      <td>1.542439</td>\n",
       "      <td>-0.883813</td>\n",
       "      <td>-1.534520</td>\n",
       "      <td>-0.929437</td>\n",
       "      <td>-1.274091</td>\n",
       "      <td>1.044816</td>\n",
       "      <td>-0.632610</td>\n",
       "      <td>0.214867</td>\n",
       "      <td>-1.155989</td>\n",
       "      <td>-1.165921</td>\n",
       "      <td>1.062657</td>\n",
       "      <td>-0.462190</td>\n",
       "      <td>-0.128853</td>\n",
       "      <td>0.815702</td>\n",
       "      <td>-0.551649</td>\n",
       "      <td>-3.139936</td>\n",
       "      <td>-1.871850</td>\n",
       "      <td>0.919162</td>\n",
       "      <td>1.101761</td>\n",
       "      <td>-0.895229</td>\n",
       "      <td>-1.357762</td>\n",
       "      <td>-1.203311</td>\n",
       "      <td>0.502165</td>\n",
       "      <td>1.217481</td>\n",
       "      <td>0.817539</td>\n",
       "      <td>-1.879420</td>\n",
       "      <td>-1.866788</td>\n",
       "      <td>1.257348</td>\n",
       "      <td>-2.040793</td>\n",
       "      <td>-2.027482</td>\n",
       "      <td>-0.929774</td>\n",
       "      <td>-0.429955</td>\n",
       "      <td>-0.408179</td>\n",
       "      <td>-0.159691</td>\n",
       "      <td>home_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992092</th>\n",
       "      <td>12.064639</td>\n",
       "      <td>-2.316174</td>\n",
       "      <td>0.433132</td>\n",
       "      <td>0.848287</td>\n",
       "      <td>2.231699</td>\n",
       "      <td>0.041330</td>\n",
       "      <td>1.067682</td>\n",
       "      <td>-1.528655</td>\n",
       "      <td>1.481193</td>\n",
       "      <td>0.463667</td>\n",
       "      <td>0.484107</td>\n",
       "      <td>2.132478</td>\n",
       "      <td>-2.712916</td>\n",
       "      <td>0.415998</td>\n",
       "      <td>-0.482643</td>\n",
       "      <td>2.368090</td>\n",
       "      <td>1.818320</td>\n",
       "      <td>-1.835697</td>\n",
       "      <td>2.213761</td>\n",
       "      <td>1.021893</td>\n",
       "      <td>2.542915</td>\n",
       "      <td>2.152431</td>\n",
       "      <td>-2.292237</td>\n",
       "      <td>-0.564741</td>\n",
       "      <td>0.029556</td>\n",
       "      <td>-0.661431</td>\n",
       "      <td>1.259634</td>\n",
       "      <td>-1.538893</td>\n",
       "      <td>0.327645</td>\n",
       "      <td>1.036215</td>\n",
       "      <td>-2.276135</td>\n",
       "      <td>-1.075133</td>\n",
       "      <td>-1.057017</td>\n",
       "      <td>0.562307</td>\n",
       "      <td>1.955697</td>\n",
       "      <td>1.429669</td>\n",
       "      <td>1.815169</td>\n",
       "      <td>-2.713671</td>\n",
       "      <td>-1.202667</td>\n",
       "      <td>-0.206876</td>\n",
       "      <td>-2.066585</td>\n",
       "      <td>-1.486135</td>\n",
       "      <td>-2.249610</td>\n",
       "      <td>0.528683</td>\n",
       "      <td>1.248011</td>\n",
       "      <td>2.305110</td>\n",
       "      <td>0.196755</td>\n",
       "      <td>-3.015813</td>\n",
       "      <td>0.884716</td>\n",
       "      <td>1.782990</td>\n",
       "      <td>0.202502</td>\n",
       "      <td>0.363303</td>\n",
       "      <td>-2.203897</td>\n",
       "      <td>0.890204</td>\n",
       "      <td>1.972601</td>\n",
       "      <td>11.605200</td>\n",
       "      <td>-2.180668</td>\n",
       "      <td>0.633318</td>\n",
       "      <td>0.505193</td>\n",
       "      <td>2.121379</td>\n",
       "      <td>0.936419</td>\n",
       "      <td>1.221556</td>\n",
       "      <td>-2.522024</td>\n",
       "      <td>-0.246410</td>\n",
       "      <td>0.123592</td>\n",
       "      <td>0.472920</td>\n",
       "      <td>1.911681</td>\n",
       "      <td>-1.404507</td>\n",
       "      <td>-1.552859</td>\n",
       "      <td>0.102178</td>\n",
       "      <td>2.614211</td>\n",
       "      <td>3.470441</td>\n",
       "      <td>-0.910743</td>\n",
       "      <td>4.107990</td>\n",
       "      <td>0.561911</td>\n",
       "      <td>-0.386817</td>\n",
       "      <td>2.148362</td>\n",
       "      <td>-0.789333</td>\n",
       "      <td>0.490890</td>\n",
       "      <td>0.348217</td>\n",
       "      <td>1.412310</td>\n",
       "      <td>-0.776693</td>\n",
       "      <td>-3.695553</td>\n",
       "      <td>-2.537028</td>\n",
       "      <td>0.654647</td>\n",
       "      <td>0.038726</td>\n",
       "      <td>0.194863</td>\n",
       "      <td>-1.450742</td>\n",
       "      <td>-1.480179</td>\n",
       "      <td>-0.329070</td>\n",
       "      <td>-1.248945</td>\n",
       "      <td>0.135936</td>\n",
       "      <td>-0.373578</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>1.145546</td>\n",
       "      <td>-1.268382</td>\n",
       "      <td>-2.736194</td>\n",
       "      <td>-2.134065</td>\n",
       "      <td>-2.336878</td>\n",
       "      <td>0.312531</td>\n",
       "      <td>-1.267814</td>\n",
       "      <td>-0.952598</td>\n",
       "      <td>-0.567959</td>\n",
       "      <td>1.417181</td>\n",
       "      <td>-1.031334</td>\n",
       "      <td>-2.777047</td>\n",
       "      <td>-2.958610</td>\n",
       "      <td>-1.015094</td>\n",
       "      <td>-0.426489</td>\n",
       "      <td>1.111898</td>\n",
       "      <td>away_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992093</th>\n",
       "      <td>12.055222</td>\n",
       "      <td>-2.425069</td>\n",
       "      <td>1.339212</td>\n",
       "      <td>0.274265</td>\n",
       "      <td>0.840987</td>\n",
       "      <td>0.807082</td>\n",
       "      <td>0.384775</td>\n",
       "      <td>-3.089207</td>\n",
       "      <td>-1.255036</td>\n",
       "      <td>0.706752</td>\n",
       "      <td>3.316945</td>\n",
       "      <td>2.584242</td>\n",
       "      <td>-1.555439</td>\n",
       "      <td>-0.624218</td>\n",
       "      <td>-1.494029</td>\n",
       "      <td>0.752059</td>\n",
       "      <td>2.634155</td>\n",
       "      <td>0.029098</td>\n",
       "      <td>-2.048228</td>\n",
       "      <td>-2.345172</td>\n",
       "      <td>-0.477442</td>\n",
       "      <td>0.588966</td>\n",
       "      <td>-1.810286</td>\n",
       "      <td>-0.671132</td>\n",
       "      <td>0.670969</td>\n",
       "      <td>-4.479330</td>\n",
       "      <td>0.957108</td>\n",
       "      <td>1.567778</td>\n",
       "      <td>1.821481</td>\n",
       "      <td>1.887503</td>\n",
       "      <td>-0.304292</td>\n",
       "      <td>2.229022</td>\n",
       "      <td>-0.374285</td>\n",
       "      <td>-0.629550</td>\n",
       "      <td>-0.550464</td>\n",
       "      <td>-2.488337</td>\n",
       "      <td>-2.492236</td>\n",
       "      <td>0.004280</td>\n",
       "      <td>-0.328167</td>\n",
       "      <td>-0.137174</td>\n",
       "      <td>-1.303862</td>\n",
       "      <td>-3.461748</td>\n",
       "      <td>-0.804760</td>\n",
       "      <td>-0.413108</td>\n",
       "      <td>-1.193188</td>\n",
       "      <td>-3.587590</td>\n",
       "      <td>-2.419330</td>\n",
       "      <td>0.831607</td>\n",
       "      <td>2.357133</td>\n",
       "      <td>0.281695</td>\n",
       "      <td>-2.476574</td>\n",
       "      <td>-2.795499</td>\n",
       "      <td>-0.080093</td>\n",
       "      <td>0.062647</td>\n",
       "      <td>-2.011499</td>\n",
       "      <td>12.360092</td>\n",
       "      <td>-1.868190</td>\n",
       "      <td>1.990823</td>\n",
       "      <td>0.546306</td>\n",
       "      <td>0.372803</td>\n",
       "      <td>-1.329084</td>\n",
       "      <td>1.395726</td>\n",
       "      <td>-0.722913</td>\n",
       "      <td>-0.893829</td>\n",
       "      <td>0.215497</td>\n",
       "      <td>1.165437</td>\n",
       "      <td>3.021942</td>\n",
       "      <td>-0.188859</td>\n",
       "      <td>2.311939</td>\n",
       "      <td>0.403286</td>\n",
       "      <td>-4.008986</td>\n",
       "      <td>1.662399</td>\n",
       "      <td>0.332054</td>\n",
       "      <td>0.291516</td>\n",
       "      <td>1.172819</td>\n",
       "      <td>-1.397060</td>\n",
       "      <td>1.400592</td>\n",
       "      <td>-0.140128</td>\n",
       "      <td>-0.156016</td>\n",
       "      <td>-0.683065</td>\n",
       "      <td>-1.079926</td>\n",
       "      <td>0.426619</td>\n",
       "      <td>-0.576034</td>\n",
       "      <td>-2.142184</td>\n",
       "      <td>-0.515036</td>\n",
       "      <td>-0.909491</td>\n",
       "      <td>1.934916</td>\n",
       "      <td>0.160027</td>\n",
       "      <td>0.916388</td>\n",
       "      <td>1.269362</td>\n",
       "      <td>-1.061561</td>\n",
       "      <td>-2.717686</td>\n",
       "      <td>-0.665092</td>\n",
       "      <td>-0.671323</td>\n",
       "      <td>-2.761000</td>\n",
       "      <td>-1.823349</td>\n",
       "      <td>-1.744156</td>\n",
       "      <td>-0.979761</td>\n",
       "      <td>0.332470</td>\n",
       "      <td>0.702492</td>\n",
       "      <td>-1.032879</td>\n",
       "      <td>-1.066695</td>\n",
       "      <td>-0.612696</td>\n",
       "      <td>-0.450766</td>\n",
       "      <td>0.290344</td>\n",
       "      <td>-1.642076</td>\n",
       "      <td>-3.316316</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.375835</td>\n",
       "      <td>-3.465042</td>\n",
       "      <td>home_win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992095</th>\n",
       "      <td>11.109830</td>\n",
       "      <td>-2.853322</td>\n",
       "      <td>3.703596</td>\n",
       "      <td>-0.584121</td>\n",
       "      <td>-0.402950</td>\n",
       "      <td>-0.371390</td>\n",
       "      <td>1.595755</td>\n",
       "      <td>-1.118035</td>\n",
       "      <td>-1.088917</td>\n",
       "      <td>-0.072962</td>\n",
       "      <td>0.782687</td>\n",
       "      <td>3.074394</td>\n",
       "      <td>-0.351447</td>\n",
       "      <td>0.339711</td>\n",
       "      <td>-0.876789</td>\n",
       "      <td>0.954087</td>\n",
       "      <td>3.884860</td>\n",
       "      <td>0.323121</td>\n",
       "      <td>-0.869362</td>\n",
       "      <td>-1.514651</td>\n",
       "      <td>-1.424525</td>\n",
       "      <td>1.026468</td>\n",
       "      <td>-0.666682</td>\n",
       "      <td>-0.521699</td>\n",
       "      <td>1.419345</td>\n",
       "      <td>-2.778054</td>\n",
       "      <td>-3.370352</td>\n",
       "      <td>-0.446712</td>\n",
       "      <td>-0.978029</td>\n",
       "      <td>-0.758080</td>\n",
       "      <td>2.408722</td>\n",
       "      <td>1.658858</td>\n",
       "      <td>-2.251337</td>\n",
       "      <td>-0.820091</td>\n",
       "      <td>-0.630519</td>\n",
       "      <td>-2.159654</td>\n",
       "      <td>1.814418</td>\n",
       "      <td>0.256719</td>\n",
       "      <td>0.567236</td>\n",
       "      <td>-0.314540</td>\n",
       "      <td>-2.998560</td>\n",
       "      <td>-3.922779</td>\n",
       "      <td>0.105072</td>\n",
       "      <td>0.225998</td>\n",
       "      <td>1.222075</td>\n",
       "      <td>-1.617306</td>\n",
       "      <td>-2.339904</td>\n",
       "      <td>-0.992463</td>\n",
       "      <td>-1.548240</td>\n",
       "      <td>0.519653</td>\n",
       "      <td>-2.910805</td>\n",
       "      <td>-2.167197</td>\n",
       "      <td>0.101099</td>\n",
       "      <td>1.144445</td>\n",
       "      <td>-0.165881</td>\n",
       "      <td>12.307758</td>\n",
       "      <td>-1.719545</td>\n",
       "      <td>4.261962</td>\n",
       "      <td>0.728169</td>\n",
       "      <td>1.500838</td>\n",
       "      <td>-1.795498</td>\n",
       "      <td>2.597198</td>\n",
       "      <td>0.399761</td>\n",
       "      <td>-0.004946</td>\n",
       "      <td>-0.912142</td>\n",
       "      <td>1.097353</td>\n",
       "      <td>2.822507</td>\n",
       "      <td>-0.720652</td>\n",
       "      <td>-1.295103</td>\n",
       "      <td>-1.490739</td>\n",
       "      <td>-1.267830</td>\n",
       "      <td>3.175089</td>\n",
       "      <td>1.368422</td>\n",
       "      <td>-0.396539</td>\n",
       "      <td>-2.110602</td>\n",
       "      <td>0.679151</td>\n",
       "      <td>1.514007</td>\n",
       "      <td>-0.744474</td>\n",
       "      <td>-2.598616</td>\n",
       "      <td>-1.680172</td>\n",
       "      <td>-2.472498</td>\n",
       "      <td>1.701903</td>\n",
       "      <td>0.506631</td>\n",
       "      <td>-1.382100</td>\n",
       "      <td>-0.412576</td>\n",
       "      <td>-3.875220</td>\n",
       "      <td>1.593144</td>\n",
       "      <td>1.643325</td>\n",
       "      <td>2.150146</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>-1.395402</td>\n",
       "      <td>0.464636</td>\n",
       "      <td>-0.435683</td>\n",
       "      <td>-0.975001</td>\n",
       "      <td>-1.386601</td>\n",
       "      <td>-3.586664</td>\n",
       "      <td>-2.865959</td>\n",
       "      <td>0.395006</td>\n",
       "      <td>1.476579</td>\n",
       "      <td>2.034092</td>\n",
       "      <td>-4.892234</td>\n",
       "      <td>-0.601987</td>\n",
       "      <td>1.354383</td>\n",
       "      <td>0.830095</td>\n",
       "      <td>-0.426868</td>\n",
       "      <td>-3.654878</td>\n",
       "      <td>-2.094599</td>\n",
       "      <td>1.339425</td>\n",
       "      <td>0.285083</td>\n",
       "      <td>-2.245146</td>\n",
       "      <td>home_win</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21374 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              home_player_1_pc_1  home_player_1_pc_2  home_player_1_pc_3  \\\n",
       "match_api_id                                                               \n",
       "493017                  9.172915           -0.705596            1.028500   \n",
       "493025                  6.467731           -2.125163            3.092089   \n",
       "493027                  7.587977           -0.669761            3.351410   \n",
       "493034                  9.172915           -0.705596            1.028500   \n",
       "493040                  8.942737           -2.146976            1.394474   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                10.658392           -1.977648            0.867377   \n",
       "1992091                12.241704           -2.064268            0.575912   \n",
       "1992092                12.064639           -2.316174            0.433132   \n",
       "1992093                12.055222           -2.425069            1.339212   \n",
       "1992095                11.109830           -2.853322            3.703596   \n",
       "\n",
       "              home_player_1_pc_4  home_player_1_pc_5  home_player_2_pc_1  \\\n",
       "match_api_id                                                               \n",
       "493017                 -0.044401            1.246272            3.957784   \n",
       "493025                 -0.930974            1.172527            0.390653   \n",
       "493027                 -1.725204            0.971832           -0.659142   \n",
       "493034                 -0.044401            1.246272            3.957784   \n",
       "493040                 -1.573403           -0.305385            0.326027   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 0.480266            1.074043            2.309500   \n",
       "1992091                 0.027214            0.566339            2.941632   \n",
       "1992092                 0.848287            2.231699            0.041330   \n",
       "1992093                 0.274265            0.840987            0.807082   \n",
       "1992095                -0.584121           -0.402950           -0.371390   \n",
       "\n",
       "              home_player_2_pc_2  home_player_2_pc_3  home_player_2_pc_4  \\\n",
       "match_api_id                                                               \n",
       "493017                  1.650964           -2.348632           -0.837480   \n",
       "493025                  1.341612            0.109198           -0.418330   \n",
       "493027                  2.447278            1.853202           -0.962117   \n",
       "493034                  1.650964           -2.348632           -0.837480   \n",
       "493040                  1.666843            0.694338            0.105792   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 0.931667           -3.860209            0.540601   \n",
       "1992091                 1.263739           -2.564271           -0.317355   \n",
       "1992092                 1.067682           -1.528655            1.481193   \n",
       "1992093                 0.384775           -3.089207           -1.255036   \n",
       "1992095                 1.595755           -1.118035           -1.088917   \n",
       "\n",
       "              home_player_2_pc_5  home_player_3_pc_1  home_player_3_pc_2  \\\n",
       "match_api_id                                                               \n",
       "493017                  0.223750           -0.817702           -0.589548   \n",
       "493025                  0.921304            2.673401            1.688787   \n",
       "493027                  0.277773            0.782575            2.059493   \n",
       "493034                  0.223750           -0.817702           -0.589548   \n",
       "493040                  1.385551            1.831429            1.436093   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -0.235396            4.829365            3.813902   \n",
       "1992091                 0.251567           -0.338978            1.001774   \n",
       "1992092                 0.463667            0.484107            2.132478   \n",
       "1992093                 0.706752            3.316945            2.584242   \n",
       "1992095                -0.072962            0.782687            3.074394   \n",
       "\n",
       "              home_player_3_pc_3  home_player_3_pc_4  home_player_3_pc_5  \\\n",
       "match_api_id                                                               \n",
       "493017                  0.195433           -2.144582            1.524434   \n",
       "493025                 -0.346764            0.032250           -0.148238   \n",
       "493027                  1.311728           -0.983126            0.652947   \n",
       "493034                  0.195433           -2.144582            1.524434   \n",
       "493040                 -0.753915           -1.703966            1.116009   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -2.190143            2.896975            0.303158   \n",
       "1992091                -1.054876            0.023669            0.503527   \n",
       "1992092                -2.712916            0.415998           -0.482643   \n",
       "1992093                -1.555439           -0.624218           -1.494029   \n",
       "1992095                -0.351447            0.339711           -0.876789   \n",
       "\n",
       "              home_player_4_pc_1  home_player_4_pc_2  home_player_4_pc_3  \\\n",
       "match_api_id                                                               \n",
       "493017                  3.108730            0.633708           -1.772338   \n",
       "493025                  0.544724            0.856618           -0.234054   \n",
       "493027                  0.750721            3.078327            1.228950   \n",
       "493034                 -0.559441            1.233335            0.426201   \n",
       "493040                  1.716745            1.133649           -0.670094   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 4.363347            3.346856           -2.511394   \n",
       "1992091                 3.322061            2.191727           -1.431455   \n",
       "1992092                 2.368090            1.818320           -1.835697   \n",
       "1992093                 0.752059            2.634155            0.029098   \n",
       "1992095                 0.954087            3.884860            0.323121   \n",
       "\n",
       "              home_player_4_pc_4  home_player_4_pc_5  home_player_5_pc_1  \\\n",
       "match_api_id                                                               \n",
       "493017                  0.807727            1.684577            0.615229   \n",
       "493025                 -0.185530            1.496783            0.349558   \n",
       "493027                  0.540478            0.034199            0.361849   \n",
       "493034                  0.609910            1.364324           -0.845848   \n",
       "493040                 -1.374712            0.468374            1.822806   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 1.256390           -1.264653            0.693648   \n",
       "1992091                -0.935647           -2.106780            0.805775   \n",
       "1992092                 2.213761            1.021893            2.542915   \n",
       "1992093                -2.048228           -2.345172           -0.477442   \n",
       "1992095                -0.869362           -1.514651           -1.424525   \n",
       "\n",
       "              home_player_5_pc_2  home_player_5_pc_3  home_player_5_pc_4  \\\n",
       "match_api_id                                                               \n",
       "493017                 -0.611994           -0.845425            0.395221   \n",
       "493025                 -0.322471            0.311433           -0.720916   \n",
       "493027                  2.339354            1.689313            0.713260   \n",
       "493034                 -0.053597            0.746034           -0.515174   \n",
       "493040                  1.126330           -0.789073           -1.478142   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 2.017825           -1.753765           -2.152817   \n",
       "1992091                 2.313243           -1.001739           -0.734564   \n",
       "1992092                 2.152431           -2.292237           -0.564741   \n",
       "1992093                 0.588966           -1.810286           -0.671132   \n",
       "1992095                 1.026468           -0.666682           -0.521699   \n",
       "\n",
       "              home_player_5_pc_5  home_player_6_pc_1  home_player_6_pc_2  \\\n",
       "match_api_id                                                               \n",
       "493017                  3.107710           -0.038702           -0.551509   \n",
       "493025                  0.765089           -2.997770           -1.279240   \n",
       "493027                  0.455514           -3.759299            1.204148   \n",
       "493034                  1.763346            0.615229           -0.611994   \n",
       "493040                  0.878564            0.882744            1.703690   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -0.662817            2.162406           -2.749882   \n",
       "1992091                -1.002397            0.167600            0.748910   \n",
       "1992092                 0.029556           -0.661431            1.259634   \n",
       "1992093                 0.670969           -4.479330            0.957108   \n",
       "1992095                 1.419345           -2.778054           -3.370352   \n",
       "\n",
       "              home_player_6_pc_3  home_player_6_pc_4  home_player_6_pc_5  \\\n",
       "match_api_id                                                               \n",
       "493017                 -0.280096           -0.068082            2.645878   \n",
       "493025                  1.492689           -0.174247            1.728035   \n",
       "493027                  3.191002           -0.784951            0.913294   \n",
       "493034                 -0.845425            0.395221            3.107710   \n",
       "493040                  0.053693            0.563981            0.978446   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -4.593596            0.434768            0.841622   \n",
       "1992091                -2.441020           -0.128875            1.354357   \n",
       "1992092                -1.538893            0.327645            1.036215   \n",
       "1992093                 1.567778            1.821481            1.887503   \n",
       "1992095                -0.446712           -0.978029           -0.758080   \n",
       "\n",
       "              home_player_7_pc_1  home_player_7_pc_2  home_player_7_pc_3  \\\n",
       "match_api_id                                                               \n",
       "493017                  1.086047           -2.583583           -0.596348   \n",
       "493025                 -1.262416            0.290702            1.097804   \n",
       "493027                 -3.018176            0.545416            2.535436   \n",
       "493034                  3.108730            0.633708           -1.772338   \n",
       "493040                 -0.780143           -0.700812            0.612057   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -0.817444            1.999270           -0.449227   \n",
       "1992091                -0.992210            1.486864           -0.837892   \n",
       "1992092                -2.276135           -1.075133           -1.057017   \n",
       "1992093                -0.304292            2.229022           -0.374285   \n",
       "1992095                 2.408722            1.658858           -2.251337   \n",
       "\n",
       "              home_player_7_pc_4  home_player_7_pc_5  home_player_8_pc_1  \\\n",
       "match_api_id                                                               \n",
       "493017                 -1.915882           -0.987251           -0.845848   \n",
       "493025                  0.377514            1.681666           -1.035827   \n",
       "493027                 -0.218828            2.658318           -2.589758   \n",
       "493034                  0.807727            1.684577            1.086047   \n",
       "493040                  1.416142            2.059364           -0.691788   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -0.166485           -0.336412            1.345606   \n",
       "1992091                -0.283414            0.885329           -0.144093   \n",
       "1992092                 0.562307            1.955697            1.429669   \n",
       "1992093                -0.629550           -0.550464           -2.488337   \n",
       "1992095                -0.820091           -0.630519           -2.159654   \n",
       "\n",
       "              home_player_8_pc_2  home_player_8_pc_3  home_player_8_pc_4  \\\n",
       "match_api_id                                                               \n",
       "493017                 -0.053597            0.746034           -0.515174   \n",
       "493025                 -0.034337            1.133088            0.899750   \n",
       "493027                  0.570906            1.654008            0.345163   \n",
       "493034                 -2.583583           -0.596348           -1.915882   \n",
       "493040                 -2.016431            0.356132           -1.164456   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 0.069802           -3.170566           -0.790618   \n",
       "1992091                -1.113188           -1.698684            0.131062   \n",
       "1992092                 1.815169           -2.713671           -1.202667   \n",
       "1992093                -2.492236            0.004280           -0.328167   \n",
       "1992095                 1.814418            0.256719            0.567236   \n",
       "\n",
       "              home_player_8_pc_5  home_player_9_pc_1  home_player_9_pc_2  \\\n",
       "match_api_id                                                               \n",
       "493017                  1.763346            4.244623            1.088030   \n",
       "493025                  1.451945           -1.970162           -1.958911   \n",
       "493027                  0.580780           -3.859427           -1.870653   \n",
       "493034                 -0.987251            4.244623            1.088030   \n",
       "493040                  2.251789            0.416905           -2.210474   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 1.344050           -1.052756            0.131148   \n",
       "1992091                 0.388273           -1.074581           -2.141164   \n",
       "1992092                -0.206876           -2.066585           -1.486135   \n",
       "1992093                -0.137174           -1.303862           -3.461748   \n",
       "1992095                -0.314540           -2.998560           -3.922779   \n",
       "\n",
       "              home_player_9_pc_3  home_player_9_pc_4  home_player_9_pc_5  \\\n",
       "match_api_id                                                               \n",
       "493017                 -2.054898           -0.414998            0.270225   \n",
       "493025                  1.072887            0.806278            2.735079   \n",
       "493027                  2.659663           -0.725774            2.171793   \n",
       "493034                 -2.054898           -0.414998            0.270225   \n",
       "493040                 -0.336256            0.054563            2.588626   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -1.406108           -0.920376            1.149814   \n",
       "1992091                -1.372831            0.593159           -0.060973   \n",
       "1992092                -2.249610            0.528683            1.248011   \n",
       "1992093                -0.804760           -0.413108           -1.193188   \n",
       "1992095                 0.105072            0.225998            1.222075   \n",
       "\n",
       "              home_player_10_pc_1  home_player_10_pc_2  home_player_10_pc_3  \\\n",
       "match_api_id                                                                  \n",
       "493017                  -0.559441             1.233335             0.426201   \n",
       "493025                  -1.206227            -2.567254             0.401355   \n",
       "493027                  -0.199689            -2.415793            -0.317491   \n",
       "493034                  -0.105750             0.211866             0.619067   \n",
       "493040                   1.923084            -2.893726            -2.042800   \n",
       "...                           ...                  ...                  ...   \n",
       "1992089                 -0.756354            -0.555587            -0.498005   \n",
       "1992091                 -1.374814            -2.597658            -1.353337   \n",
       "1992092                  2.305110             0.196755            -3.015813   \n",
       "1992093                 -3.587590            -2.419330             0.831607   \n",
       "1992095                 -1.617306            -2.339904            -0.992463   \n",
       "\n",
       "              home_player_10_pc_4  home_player_10_pc_5  home_player_11_pc_1  \\\n",
       "match_api_id                                                                  \n",
       "493017                   0.609910             1.364324             1.472274   \n",
       "493025                  -0.240084             1.552955            -0.417405   \n",
       "493027                   0.067180             0.943378            -1.056848   \n",
       "493034                   0.766076            -0.979514             1.472274   \n",
       "493040                  -1.018973            -1.292926             1.464444   \n",
       "...                           ...                  ...                  ...   \n",
       "1992089                  0.934758            -2.606863            -2.571317   \n",
       "1992091                 -0.079045             0.449154             1.613120   \n",
       "1992092                  0.884716             1.782990             0.202502   \n",
       "1992093                  2.357133             0.281695            -2.476574   \n",
       "1992095                 -1.548240             0.519653            -2.910805   \n",
       "\n",
       "              home_player_11_pc_2  home_player_11_pc_3  home_player_11_pc_4  \\\n",
       "match_api_id                                                                  \n",
       "493017                  -0.298277            -1.530989             0.743621   \n",
       "493025                  -0.845298             0.077456            -1.026087   \n",
       "493027                  -1.205950             1.067483             1.246078   \n",
       "493034                  -0.298277            -1.530989             0.743621   \n",
       "493040                   0.379332            -0.902796            -0.837762   \n",
       "...                           ...                  ...                  ...   \n",
       "1992089                 -2.788252            -0.783057            -0.228549   \n",
       "1992091                 -2.543332            -2.596794             1.483082   \n",
       "1992092                  0.363303            -2.203897             0.890204   \n",
       "1992093                 -2.795499            -0.080093             0.062647   \n",
       "1992095                 -2.167197             0.101099             1.144445   \n",
       "\n",
       "              home_player_11_pc_5  away_player_1_pc_1  away_player_1_pc_2  \\\n",
       "match_api_id                                                                \n",
       "493017                  -0.875547            9.794795           -0.549117   \n",
       "493025                  -0.249720            6.746068           -1.225452   \n",
       "493027                  -0.070047            8.942737           -2.146976   \n",
       "493034                  -0.875547            7.587977           -0.669761   \n",
       "493040                   0.211743            9.554376           -0.788436   \n",
       "...                           ...                 ...                 ...   \n",
       "1992089                  0.825315           11.297455           -1.943311   \n",
       "1992091                  0.105411           12.290704           -2.430347   \n",
       "1992092                  1.972601           11.605200           -2.180668   \n",
       "1992093                 -2.011499           12.360092           -1.868190   \n",
       "1992095                 -0.165881           12.307758           -1.719545   \n",
       "\n",
       "              away_player_1_pc_3  away_player_1_pc_4  away_player_1_pc_5  \\\n",
       "match_api_id                                                               \n",
       "493017                  1.941560            0.281992            0.521328   \n",
       "493025                  5.441467           -1.741143            0.153029   \n",
       "493027                  1.394474           -1.573403           -0.305385   \n",
       "493034                  3.351410           -1.725204            0.971832   \n",
       "493040                  2.277353            0.709577            0.731361   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -0.622608            1.229927            2.987196   \n",
       "1992091                -1.144252           -0.197618            0.806754   \n",
       "1992092                 0.633318            0.505193            2.121379   \n",
       "1992093                 1.990823            0.546306            0.372803   \n",
       "1992095                 4.261962            0.728169            1.500838   \n",
       "\n",
       "              away_player_2_pc_1  away_player_2_pc_2  away_player_2_pc_3  \\\n",
       "match_api_id                                                               \n",
       "493017                 -1.886782            1.005850            1.291888   \n",
       "493025                 -0.720625            1.362837            0.950063   \n",
       "493027                  0.326027            1.666843            0.694338   \n",
       "493034                 -0.659142            2.447278            1.853202   \n",
       "493040                  1.970444            1.864556           -0.597320   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 0.715893            1.452546           -2.111059   \n",
       "1992091                 4.561174            1.116819           -3.853639   \n",
       "1992092                 0.936419            1.221556           -2.522024   \n",
       "1992093                -1.329084            1.395726           -0.722913   \n",
       "1992095                -1.795498            2.597198            0.399761   \n",
       "\n",
       "              away_player_2_pc_4  away_player_2_pc_5  away_player_3_pc_1  \\\n",
       "match_api_id                                                               \n",
       "493017                  0.172012            1.212622            1.320201   \n",
       "493025                 -0.430052            1.920381            1.321520   \n",
       "493027                  0.105792            1.385551            1.716745   \n",
       "493034                 -0.962117            0.277773            0.361849   \n",
       "493040                  0.168243            0.246152            4.470705   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -0.524585            0.158115            5.742015   \n",
       "1992091                -1.449020           -0.008049            0.859240   \n",
       "1992092                -0.246410            0.123592            0.472920   \n",
       "1992093                -0.893829            0.215497            1.165437   \n",
       "1992095                -0.004946           -0.912142            1.097353   \n",
       "\n",
       "              away_player_3_pc_2  away_player_3_pc_3  away_player_3_pc_4  \\\n",
       "match_api_id                                                               \n",
       "493017                  1.065244           -0.406996           -1.803778   \n",
       "493025                  2.234319            0.546943            0.392576   \n",
       "493027                  1.133649           -0.670094           -1.374712   \n",
       "493034                  2.339354            1.689313            0.713260   \n",
       "493040                  2.449109           -1.396639           -0.315235   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 3.143138           -3.280495            0.901933   \n",
       "1992091                 1.593152           -1.523836            0.182434   \n",
       "1992092                 1.911681           -1.404507           -1.552859   \n",
       "1992093                 3.021942           -0.188859            2.311939   \n",
       "1992095                 2.822507           -0.720652           -1.295103   \n",
       "\n",
       "              away_player_3_pc_5  away_player_4_pc_1  away_player_4_pc_2  \\\n",
       "match_api_id                                                               \n",
       "493017                  0.573483            2.628391            0.940615   \n",
       "493025                  1.081937           -1.157135            1.181325   \n",
       "493027                  0.468374            1.822806            1.126330   \n",
       "493034                  0.455514            1.052194            2.080899   \n",
       "493040                  0.008914            1.829497            1.079852   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -1.055971            3.559549            2.023672   \n",
       "1992091                 0.923206            3.366277            2.096648   \n",
       "1992092                 0.102178            2.614211            3.470441   \n",
       "1992093                 0.403286           -4.008986            1.662399   \n",
       "1992095                -1.490739           -1.267830            3.175089   \n",
       "\n",
       "              away_player_4_pc_3  away_player_4_pc_4  away_player_4_pc_5  \\\n",
       "match_api_id                                                               \n",
       "493017                 -0.609534           -1.586848            0.652795   \n",
       "493025                  0.901247            0.722505            0.967595   \n",
       "493027                 -0.789073           -1.478142            0.878564   \n",
       "493034                  0.370792           -0.782729            0.394531   \n",
       "493040                 -2.363541           -0.545278            0.775451   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -2.954817            1.214646            0.395441   \n",
       "1992091                -2.560049            1.302320            0.621418   \n",
       "1992092                -0.910743            4.107990            0.561911   \n",
       "1992093                 0.332054            0.291516            1.172819   \n",
       "1992095                 1.368422           -0.396539           -2.110602   \n",
       "\n",
       "              away_player_5_pc_1  away_player_5_pc_2  away_player_5_pc_3  \\\n",
       "match_api_id                                                               \n",
       "493017                  3.207876            0.685128           -1.480756   \n",
       "493025                 -1.573349            0.366277            1.085719   \n",
       "493027                  5.021780            0.645976           -1.954515   \n",
       "493034                  2.077104            1.903504            0.684877   \n",
       "493040                  4.019384            0.341251           -2.730811   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -0.652392            0.617508           -2.119288   \n",
       "1992091                -0.179065            1.542439           -0.883813   \n",
       "1992092                -0.386817            2.148362           -0.789333   \n",
       "1992093                -1.397060            1.400592           -0.140128   \n",
       "1992095                 0.679151            1.514007           -0.744474   \n",
       "\n",
       "              away_player_5_pc_4  away_player_5_pc_5  away_player_6_pc_1  \\\n",
       "match_api_id                                                               \n",
       "493017                 -1.499414            0.134771           -1.360311   \n",
       "493025                 -0.362088            0.794429           -2.362155   \n",
       "493027                 -1.642732            0.230055            0.882744   \n",
       "493034                  1.471026           -0.492695           -3.759299   \n",
       "493040                 -1.093104            0.247451            0.682845   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 0.420918           -0.312378           -1.179215   \n",
       "1992091                -1.534520           -0.929437           -1.274091   \n",
       "1992092                 0.490890            0.348217            1.412310   \n",
       "1992093                -0.156016           -0.683065           -1.079926   \n",
       "1992095                -2.598616           -1.680172           -2.472498   \n",
       "\n",
       "              away_player_6_pc_2  away_player_6_pc_3  away_player_6_pc_4  \\\n",
       "match_api_id                                                               \n",
       "493017                 -2.979972            1.181522           -0.726996   \n",
       "493025                  0.960996            2.341796            0.480765   \n",
       "493027                  1.703690            0.053693            0.563981   \n",
       "493034                  1.204148            3.191002           -0.784951   \n",
       "493040                  0.218515           -0.581395            0.951872   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 0.990786           -0.758331            0.280326   \n",
       "1992091                 1.044816           -0.632610            0.214867   \n",
       "1992092                -0.776693           -3.695553           -2.537028   \n",
       "1992093                 0.426619           -0.576034           -2.142184   \n",
       "1992095                 1.701903            0.506631           -1.382100   \n",
       "\n",
       "              away_player_6_pc_5  away_player_7_pc_1  away_player_7_pc_2  \\\n",
       "match_api_id                                                               \n",
       "493017                  1.161218           -1.735000           -0.323721   \n",
       "493025                  1.235871           -2.036662            1.423269   \n",
       "493027                  0.978446           -0.780143           -0.700812   \n",
       "493034                  0.913294           -0.631336           -1.917288   \n",
       "493040                  2.794600           -0.793495           -0.740710   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -1.019536           -0.983237            1.033960   \n",
       "1992091                -1.155989           -1.165921            1.062657   \n",
       "1992092                 0.654647            0.038726            0.194863   \n",
       "1992093                -0.515036           -0.909491            1.934916   \n",
       "1992095                -0.412576           -3.875220            1.593144   \n",
       "\n",
       "              away_player_7_pc_3  away_player_7_pc_4  away_player_7_pc_5  \\\n",
       "match_api_id                                                               \n",
       "493017                  0.977427           -1.047350            1.307837   \n",
       "493025                  1.975662            0.456836            0.564672   \n",
       "493027                  0.612057            1.416142            2.059364   \n",
       "493034                  0.404582           -1.604514           -0.334234   \n",
       "493040                  0.492791            0.151706            2.489086   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -0.714609           -0.027433            0.645499   \n",
       "1992091                -0.462190           -0.128853            0.815702   \n",
       "1992092                -1.450742           -1.480179           -0.329070   \n",
       "1992093                 0.160027            0.916388            1.269362   \n",
       "1992095                 1.643325            2.150146            2.170000   \n",
       "\n",
       "              away_player_8_pc_1  away_player_8_pc_2  away_player_8_pc_3  \\\n",
       "match_api_id                                                               \n",
       "493017                 -0.004187            1.646099            0.543917   \n",
       "493025                 -1.002934           -2.335553            0.470491   \n",
       "493027                 -0.691788           -2.016431            0.356132   \n",
       "493034                 -3.859427           -1.870653            2.659663   \n",
       "493040                  1.933672            0.891587           -1.109525   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -0.468185           -2.492838           -2.135542   \n",
       "1992091                -0.551649           -3.139936           -1.871850   \n",
       "1992092                -1.248945            0.135936           -0.373578   \n",
       "1992093                -1.061561           -2.717686           -0.665092   \n",
       "1992095                -1.395402            0.464636           -0.435683   \n",
       "\n",
       "              away_player_8_pc_4  away_player_8_pc_5  away_player_9_pc_1  \\\n",
       "match_api_id                                                               \n",
       "493017                  0.101722            0.374692           -1.836650   \n",
       "493025                 -0.111478            0.585989           -1.407944   \n",
       "493027                 -1.164456            2.251789            1.464444   \n",
       "493034                 -0.725774            2.171793           -3.018176   \n",
       "493040                 -0.792216           -0.402166           -0.203211   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                 0.672037            1.175961            0.089223   \n",
       "1992091                 0.919162            1.101761           -0.895229   \n",
       "1992092                -0.008843            1.145546           -1.268382   \n",
       "1992093                -0.671323           -2.761000           -1.823349   \n",
       "1992095                -0.975001           -1.386601           -3.586664   \n",
       "\n",
       "              away_player_9_pc_2  away_player_9_pc_3  away_player_9_pc_4  \\\n",
       "match_api_id                                                               \n",
       "493017                 -2.551881            1.212316            0.139711   \n",
       "493025                 -2.205414            1.069296            0.053596   \n",
       "493027                  0.379332           -0.902796           -0.837762   \n",
       "493034                  0.545416            2.535436           -0.218828   \n",
       "493040                 -1.625096           -0.009942            0.220891   \n",
       "...                          ...                 ...                 ...   \n",
       "1992089                -1.409898           -2.184536            0.860498   \n",
       "1992091                -1.357762           -1.203311            0.502165   \n",
       "1992092                -2.736194           -2.134065           -2.336878   \n",
       "1992093                -1.744156           -0.979761            0.332470   \n",
       "1992095                -2.865959            0.395006            1.476579   \n",
       "\n",
       "              away_player_9_pc_5  away_player_10_pc_1  away_player_10_pc_2  \\\n",
       "match_api_id                                                                 \n",
       "493017                  1.155972            -0.916797            -1.202214   \n",
       "493025                  0.809813            -2.259696            -2.778135   \n",
       "493027                  0.211743             1.923084            -2.893726   \n",
       "493034                  2.658318            -2.589758             0.570906   \n",
       "493040                  1.091029             0.344323            -3.281820   \n",
       "...                          ...                  ...                  ...   \n",
       "1992089                 2.273182            -1.604118            -1.071515   \n",
       "1992091                 1.217481             0.817539            -1.879420   \n",
       "1992092                 0.312531            -1.267814            -0.952598   \n",
       "1992093                 0.702492            -1.032879            -1.066695   \n",
       "1992095                 2.034092            -4.892234            -0.601987   \n",
       "\n",
       "              away_player_10_pc_3  away_player_10_pc_4  away_player_10_pc_5  \\\n",
       "match_api_id                                                                  \n",
       "493017                   0.022668            -0.118531            -0.332637   \n",
       "493025                   1.816538             1.407815             0.022775   \n",
       "493027                  -2.042800            -1.018973            -1.292926   \n",
       "493034                   1.654008             0.345163             0.580780   \n",
       "493040                  -1.047589            -0.870665             0.278254   \n",
       "...                           ...                  ...                  ...   \n",
       "1992089                 -1.719817            -0.597994             0.268359   \n",
       "1992091                 -1.866788             1.257348            -2.040793   \n",
       "1992092                 -0.567959             1.417181            -1.031334   \n",
       "1992093                 -0.612696            -0.450766             0.290344   \n",
       "1992095                  1.354383             0.830095            -0.426868   \n",
       "\n",
       "              away_player_11_pc_1  away_player_11_pc_2  away_player_11_pc_3  \\\n",
       "match_api_id                                                                  \n",
       "493017                   0.628660            -1.751083            -0.030693   \n",
       "493025                   0.169010            -2.319129             0.361658   \n",
       "493027                   0.416905            -2.210474            -0.336256   \n",
       "493034                  -1.056848            -1.205950             1.067483   \n",
       "493040                   0.817798            -2.420838            -0.361229   \n",
       "...                           ...                  ...                  ...   \n",
       "1992089                 -2.027482            -0.929774            -0.429955   \n",
       "1992091                 -2.027482            -0.929774            -0.429955   \n",
       "1992092                 -2.777047            -2.958610            -1.015094   \n",
       "1992093                 -1.642076            -3.316316             0.011961   \n",
       "1992095                 -3.654878            -2.094599             1.339425   \n",
       "\n",
       "              away_player_11_pc_4  away_player_11_pc_5 match_result  \n",
       "match_api_id                                                         \n",
       "493017                   0.418133            -0.206630     home_win  \n",
       "493025                   0.935315            -1.877390     away_win  \n",
       "493027                   0.054563             2.588626     home_win  \n",
       "493034                   1.246078            -0.070047     home_win  \n",
       "493040                   0.689072            -0.850103         draw  \n",
       "...                           ...                  ...          ...  \n",
       "1992089                 -0.408179            -0.159691         draw  \n",
       "1992091                 -0.408179            -0.159691     home_win  \n",
       "1992092                 -0.426489             1.111898     away_win  \n",
       "1992093                  0.375835            -3.465042     home_win  \n",
       "1992095                  0.285083            -2.245146     home_win  \n",
       "\n",
       "[21374 rows x 111 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_match_player_attr_pcs = df_match_player_attr_pcs.set_index(\"match_api_id\")\n",
    "df_match_player_attr_pcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145       493017\n",
       "153       493025\n",
       "155       493027\n",
       "162       493034\n",
       "168       493040\n",
       "          ...   \n",
       "24172    1778119\n",
       "24173    1778120\n",
       "24174    1778121\n",
       "24175    1778122\n",
       "24176    1778123\n",
       "Name: match_api_id, Length: 17029, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_match_api_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bool = df_match_player_attr_pcs.reset_index().match_api_id.isin(train_match_api_id)\n",
    "test_bool = df_match_player_attr_pcs.reset_index().match_api_id.isin(test_match_api_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc_train = df_match_player_attr_pcs.reset_index()[train_bool].set_index(\"match_api_id\")\n",
    "df_pc_test = df_match_player_attr_pcs.reset_index()[test_bool].set_index(\"match_api_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pc_train = df_pc_train.drop(\"match_result\", axis = 1)\n",
    "y_pc_train = df_pc_train.match_result \n",
    "\n",
    "X_pc_test = df_pc_test.drop(\"match_result\", axis = 1)\n",
    "y_pc_test = df_pc_test.match_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data:  17029\n",
      "Number of test data:  2662\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train data: \", X_pc_train.shape[0])\n",
    "print(\"Number of test data: \", X_pc_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"KNN\", \n",
    "         \"LDA\", \n",
    "         \"QDA\", \n",
    "         \"Naive Bayes\",\n",
    "         \"Logistic regression\",\n",
    "         \"Decesion tree\", \n",
    "         \"Random Forest\",  \n",
    "         \"AdaBoost\",\n",
    "         \"Polynomial kernel SVM\",\n",
    "         \"Radial kernel SVM\",\n",
    "         \"GBM\"\n",
    "        ]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    GaussianNB(), \n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(random_state = 42),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    SVC(kernel = \"poly\"),\n",
    "    SVC(kernel = \"rbf\"),\n",
    "    GradientBoostingClassifier()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_accuracy = pd.DataFrame(names, columns = [\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dict = {}\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_pc_train, y_pc_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_pc_test)\n",
    "    y_pred_dict[name] = y_pred\n",
    "    \n",
    "    accuracy = np.mean(y_pred == y_pc_test)\n",
    "    \n",
    "    result_accuracy.loc[result_accuracy.model_name == name, \"Player PC Variables\"] = round(accuracy * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>Player PC Variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>41.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>50.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QDA</td>\n",
       "      <td>42.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>46.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>50.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decesion tree</td>\n",
       "      <td>38.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>49.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>49.624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polynomial kernel SVM</td>\n",
       "      <td>50.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Radial kernel SVM</td>\n",
       "      <td>50.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GBM</td>\n",
       "      <td>50.150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  Player PC Variables\n",
       "0                     KNN               41.134\n",
       "1                     LDA               50.075\n",
       "2                     QDA               42.637\n",
       "3             Naive Bayes               46.920\n",
       "4     Logistic regression               50.225\n",
       "5           Decesion tree               38.467\n",
       "6           Random Forest               49.775\n",
       "7                AdaBoost               49.624\n",
       "8   Polynomial kernel SVM               50.413\n",
       "9       Radial kernel SVM               50.864\n",
       "10                    GBM               50.150"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Variable set 2: Betting information features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_betting_stat = df_match_betting_stat.merge(df_match_basic[[\"match_api_id\", \"match_result\"]], how = \"left\", on = \"match_api_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bool = df_match_betting_stat.match_api_id.isin(train_match_api_id)\n",
    "test_bool = df_match_betting_stat.match_api_id.isin(test_match_api_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bet_train = df_match_betting_stat[train_bool].set_index(\"match_api_id\")\n",
    "df_bet_test = df_match_betting_stat[test_bool].set_index(\"match_api_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bet_train = df_bet_train.drop(\"match_result\", axis = 1)\n",
    "y_bet_train = df_bet_train.match_result \n",
    "\n",
    "X_bet_test = df_bet_test.drop(\"match_result\", axis = 1)\n",
    "y_bet_test = df_bet_test.match_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data:  17029\n",
      "Number of test data:  2662\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train data: \", X_bet_train.shape[0])\n",
    "print(\"Number of test data: \", X_bet_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_bet_train, y_bet_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_bet_test)\n",
    "    y_pred_dict[name] = y_pred\n",
    "    \n",
    "    accuracy = np.mean(y_pred == y_bet_test)\n",
    "    \n",
    "    result_accuracy.loc[result_accuracy.model_name == name, \"Betting Statistics Variables\"] = round(accuracy * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>Player PC Variables</th>\n",
       "      <th>Betting Statistics Variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>41.134</td>\n",
       "      <td>45.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>50.075</td>\n",
       "      <td>51.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QDA</td>\n",
       "      <td>42.637</td>\n",
       "      <td>33.959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>46.920</td>\n",
       "      <td>41.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>50.225</td>\n",
       "      <td>51.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decesion tree</td>\n",
       "      <td>38.467</td>\n",
       "      <td>42.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>49.775</td>\n",
       "      <td>48.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>49.624</td>\n",
       "      <td>51.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polynomial kernel SVM</td>\n",
       "      <td>50.413</td>\n",
       "      <td>51.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Radial kernel SVM</td>\n",
       "      <td>50.864</td>\n",
       "      <td>51.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GBM</td>\n",
       "      <td>50.150</td>\n",
       "      <td>51.803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  Player PC Variables  Betting Statistics Variables\n",
       "0                     KNN               41.134                        45.304\n",
       "1                     LDA               50.075                        51.503\n",
       "2                     QDA               42.637                        33.959\n",
       "3             Naive Bayes               46.920                        41.998\n",
       "4     Logistic regression               50.225                        51.728\n",
       "5           Decesion tree               38.467                        42.299\n",
       "6           Random Forest               49.775                        48.009\n",
       "7                AdaBoost               49.624                        51.653\n",
       "8   Polynomial kernel SVM               50.413                        51.014\n",
       "9       Radial kernel SVM               50.864                        51.427\n",
       "10                    GBM               50.150                        51.803"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Variable set 3: Team attribute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_team_num_attr = df_match_team_num_attr.merge(df_match_basic[[\"match_api_id\", \"match_result\"]], how = \"left\", on = \"match_api_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bool = df_match_team_num_attr.match_api_id.isin(train_match_api_id)\n",
    "test_bool = df_match_team_num_attr.match_api_id.isin(test_match_api_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_team_train = df_match_team_num_attr[train_bool].set_index(\"match_api_id\")\n",
    "df_team_test = df_match_team_num_attr[test_bool].set_index(\"match_api_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_team_train = df_team_train.drop(\"match_result\", axis = 1)\n",
    "y_team_train = df_team_train.match_result \n",
    "\n",
    "X_team_test = df_team_test.drop(\"match_result\", axis = 1)\n",
    "y_team_test = df_team_test.match_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data:  17029\n",
      "Number of test data:  2662\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train data: \", X_team_train.shape[0])\n",
    "print(\"Number of test data: \", X_team_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_team_train.fillna(0, inplace = True)\n",
    "X_team_test.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_team_train, y_team_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_team_test)\n",
    "    y_pred_dict[name] = y_pred\n",
    "    \n",
    "    accuracy = np.mean(y_pred == y_team_test)\n",
    "    \n",
    "    result_accuracy.loc[result_accuracy.model_name == name, \"Team attribute Variables\"] = round(accuracy * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>Player PC Variables</th>\n",
       "      <th>Betting Statistics Variables</th>\n",
       "      <th>Team attribute Variables</th>\n",
       "      <th>Team's goal and win percentage rolling Variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>41.134</td>\n",
       "      <td>45.304</td>\n",
       "      <td>38.881</td>\n",
       "      <td>40.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>50.075</td>\n",
       "      <td>51.503</td>\n",
       "      <td>45.680</td>\n",
       "      <td>49.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QDA</td>\n",
       "      <td>42.637</td>\n",
       "      <td>33.959</td>\n",
       "      <td>45.605</td>\n",
       "      <td>45.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>46.920</td>\n",
       "      <td>41.998</td>\n",
       "      <td>46.319</td>\n",
       "      <td>47.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>50.225</td>\n",
       "      <td>51.728</td>\n",
       "      <td>45.642</td>\n",
       "      <td>50.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decesion tree</td>\n",
       "      <td>38.467</td>\n",
       "      <td>42.299</td>\n",
       "      <td>37.716</td>\n",
       "      <td>40.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>49.775</td>\n",
       "      <td>48.009</td>\n",
       "      <td>44.591</td>\n",
       "      <td>48.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>49.624</td>\n",
       "      <td>51.653</td>\n",
       "      <td>47.333</td>\n",
       "      <td>50.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polynomial kernel SVM</td>\n",
       "      <td>50.413</td>\n",
       "      <td>51.014</td>\n",
       "      <td>45.192</td>\n",
       "      <td>50.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Radial kernel SVM</td>\n",
       "      <td>50.864</td>\n",
       "      <td>51.427</td>\n",
       "      <td>45.004</td>\n",
       "      <td>49.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GBM</td>\n",
       "      <td>50.150</td>\n",
       "      <td>51.803</td>\n",
       "      <td>47.521</td>\n",
       "      <td>49.587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  Player PC Variables  Betting Statistics Variables  \\\n",
       "0                     KNN               41.134                        45.304   \n",
       "1                     LDA               50.075                        51.503   \n",
       "2                     QDA               42.637                        33.959   \n",
       "3             Naive Bayes               46.920                        41.998   \n",
       "4     Logistic regression               50.225                        51.728   \n",
       "5           Decesion tree               38.467                        42.299   \n",
       "6           Random Forest               49.775                        48.009   \n",
       "7                AdaBoost               49.624                        51.653   \n",
       "8   Polynomial kernel SVM               50.413                        51.014   \n",
       "9       Radial kernel SVM               50.864                        51.427   \n",
       "10                    GBM               50.150                        51.803   \n",
       "\n",
       "    Team attribute Variables  Team's goal and win percentage rolling Variables  \n",
       "0                     38.881                                            40.909  \n",
       "1                     45.680                                            49.850  \n",
       "2                     45.605                                            45.417  \n",
       "3                     46.319                                            47.220  \n",
       "4                     45.642                                            50.301  \n",
       "5                     37.716                                            40.947  \n",
       "6                     44.591                                            48.573  \n",
       "7                     47.333                                            50.301  \n",
       "8                     45.192                                            50.000  \n",
       "9                     45.004                                            49.699  \n",
       "10                    47.521                                            49.587  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Variable set 4: Goal and win percentage rolling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_team_win_goal_rolling_features = df_team_win_goal_rolling_features.merge(df_match_basic[[\"match_api_id\", \"match_result\"]], how = \"left\", on = \"match_api_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bool = df_team_win_goal_rolling_features.reset_index().match_api_id.isin(train_match_api_id)\n",
    "test_bool = df_team_win_goal_rolling_features.reset_index().match_api_id.isin(test_match_api_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolling_train = df_team_win_goal_rolling_features[train_bool].set_index(\"match_api_id\")\n",
    "df_rolling_test = df_team_win_goal_rolling_features[test_bool].set_index(\"match_api_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rolling_train = df_rolling_train.drop(\"match_result\", axis = 1)\n",
    "y_rolling_train = df_rolling_train.match_result \n",
    "\n",
    "X_rolling_test = df_rolling_test.drop(\"match_result\", axis = 1)\n",
    "y_rolling_test = df_rolling_test.match_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data:  17029\n",
      "Number of test data:  2662\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train data: \", X_rolling_train.shape[0])\n",
    "print(\"Number of test data: \", X_rolling_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rolling_train.fillna(0, inplace = True)\n",
    "X_rolling_test.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_rolling_train, y_rolling_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_rolling_test)\n",
    "    y_pred_dict[name] = y_pred\n",
    "    \n",
    "    accuracy = np.mean(y_pred == y_rolling_test)\n",
    "    \n",
    "    result_accuracy.loc[result_accuracy.model_name == name, \"Team's goal and win percentage rolling Variables\"] = round(accuracy * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>Player PC Variables</th>\n",
       "      <th>Betting Statistics Variables</th>\n",
       "      <th>Team attribute Variables</th>\n",
       "      <th>Team's goal and win percentage rolling Variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>41.134</td>\n",
       "      <td>45.304</td>\n",
       "      <td>38.881</td>\n",
       "      <td>40.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>50.075</td>\n",
       "      <td>51.503</td>\n",
       "      <td>45.455</td>\n",
       "      <td>49.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QDA</td>\n",
       "      <td>42.637</td>\n",
       "      <td>33.959</td>\n",
       "      <td>45.567</td>\n",
       "      <td>45.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>46.920</td>\n",
       "      <td>41.998</td>\n",
       "      <td>46.168</td>\n",
       "      <td>47.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>50.225</td>\n",
       "      <td>51.728</td>\n",
       "      <td>45.680</td>\n",
       "      <td>50.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decesion tree</td>\n",
       "      <td>38.467</td>\n",
       "      <td>42.299</td>\n",
       "      <td>37.716</td>\n",
       "      <td>40.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>49.775</td>\n",
       "      <td>48.009</td>\n",
       "      <td>45.379</td>\n",
       "      <td>48.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>49.624</td>\n",
       "      <td>51.653</td>\n",
       "      <td>47.333</td>\n",
       "      <td>50.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polynomial kernel SVM</td>\n",
       "      <td>50.413</td>\n",
       "      <td>51.014</td>\n",
       "      <td>44.515</td>\n",
       "      <td>50.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Radial kernel SVM</td>\n",
       "      <td>50.864</td>\n",
       "      <td>51.427</td>\n",
       "      <td>44.515</td>\n",
       "      <td>49.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GBM</td>\n",
       "      <td>50.150</td>\n",
       "      <td>51.803</td>\n",
       "      <td>47.596</td>\n",
       "      <td>49.587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  Player PC Variables  Betting Statistics Variables  \\\n",
       "0                     KNN               41.134                        45.304   \n",
       "1                     LDA               50.075                        51.503   \n",
       "2                     QDA               42.637                        33.959   \n",
       "3             Naive Bayes               46.920                        41.998   \n",
       "4     Logistic regression               50.225                        51.728   \n",
       "5           Decesion tree               38.467                        42.299   \n",
       "6           Random Forest               49.775                        48.009   \n",
       "7                AdaBoost               49.624                        51.653   \n",
       "8   Polynomial kernel SVM               50.413                        51.014   \n",
       "9       Radial kernel SVM               50.864                        51.427   \n",
       "10                    GBM               50.150                        51.803   \n",
       "\n",
       "    Team attribute Variables  Team's goal and win percentage rolling Variables  \n",
       "0                     38.881                                            40.909  \n",
       "1                     45.455                                            49.850  \n",
       "2                     45.567                                            45.417  \n",
       "3                     46.168                                            47.220  \n",
       "4                     45.680                                            50.301  \n",
       "5                     37.716                                            40.947  \n",
       "6                     45.379                                            48.573  \n",
       "7                     47.333                                            50.301  \n",
       "8                     44.515                                            50.000  \n",
       "9                     44.515                                            49.699  \n",
       "10                    47.596                                            49.587  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Use all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_match_player_attr_pcs.merge(df_match_betting_stat.drop(\"match_result\", axis = 1), how = \"left\", on = [\"match_api_id\"]) \\\n",
    "                                 .merge(df_match_team_num_attr.drop(\"match_result\", axis = 1), how = \"left\", on = [\"match_api_id\"]) \\\n",
    "                                 .merge(df_team_win_goal_rolling_features.drop(\"match_result\", axis = 1), how = \"left\", on = [\"match_api_id\"]) \n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bool = df_all.match_api_id.isin(train_match_api_id)\n",
    "test_bool = df_all.match_api_id.isin(test_match_api_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_train = df_all[train_bool].set_index(\"match_api_id\")\n",
    "df_all_test = df_all[test_bool].set_index(\"match_api_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_train = df_all_train.drop(\"match_result\", axis = 1)\n",
    "y_all_train = df_all_train.match_result \n",
    "\n",
    "X_all_test = df_all_test.drop(\"match_result\", axis = 1)\n",
    "y_all_test = df_all_test.match_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data:  17029\n",
      "Number of test data:  2662\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train data: \", X_all_train.shape[0])\n",
    "print(\"Number of test data: \", X_all_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_train.fillna(0, inplace = True)\n",
    "X_all_test.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalter = StandardScaler()\n",
    "scalter.fit(X_all_train)\n",
    "X_all_train_std = scalter.transform(X_all_train)\n",
    "X_all_test_std = scalter.transform(X_all_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_all_train_std, y_all_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_all_test_std)\n",
    "    y_pred_dict[name] = y_pred\n",
    "    \n",
    "    accuracy = np.mean(y_pred == y_all_test)\n",
    "    \n",
    "    result_accuracy.loc[result_accuracy.model_name == name, \"All Variables std\"] = round(accuracy * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>Player PC Variables</th>\n",
       "      <th>Betting Statistics Variables</th>\n",
       "      <th>Team attribute Variables</th>\n",
       "      <th>Team's goal and win percentage rolling Variables</th>\n",
       "      <th>All Variables</th>\n",
       "      <th>Baseline accuracy</th>\n",
       "      <th>All Variables std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>41.134</td>\n",
       "      <td>45.304</td>\n",
       "      <td>38.881</td>\n",
       "      <td>40.909</td>\n",
       "      <td>41.773</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>43.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>50.075</td>\n",
       "      <td>51.503</td>\n",
       "      <td>45.680</td>\n",
       "      <td>49.850</td>\n",
       "      <td>50.789</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QDA</td>\n",
       "      <td>42.637</td>\n",
       "      <td>33.959</td>\n",
       "      <td>45.605</td>\n",
       "      <td>45.417</td>\n",
       "      <td>43.050</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>44.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>46.920</td>\n",
       "      <td>41.998</td>\n",
       "      <td>46.319</td>\n",
       "      <td>47.220</td>\n",
       "      <td>46.243</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>46.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>50.225</td>\n",
       "      <td>51.728</td>\n",
       "      <td>45.642</td>\n",
       "      <td>50.301</td>\n",
       "      <td>51.390</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>51.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decesion tree</td>\n",
       "      <td>38.467</td>\n",
       "      <td>42.299</td>\n",
       "      <td>37.716</td>\n",
       "      <td>40.947</td>\n",
       "      <td>40.759</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>40.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>49.775</td>\n",
       "      <td>48.009</td>\n",
       "      <td>44.591</td>\n",
       "      <td>48.573</td>\n",
       "      <td>50.488</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>49.624</td>\n",
       "      <td>51.653</td>\n",
       "      <td>47.333</td>\n",
       "      <td>50.301</td>\n",
       "      <td>51.277</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>51.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polynomial kernel SVM</td>\n",
       "      <td>50.413</td>\n",
       "      <td>51.014</td>\n",
       "      <td>45.192</td>\n",
       "      <td>50.000</td>\n",
       "      <td>50.826</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>48.911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Radial kernel SVM</td>\n",
       "      <td>50.864</td>\n",
       "      <td>51.427</td>\n",
       "      <td>45.004</td>\n",
       "      <td>49.699</td>\n",
       "      <td>51.165</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>51.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GBM</td>\n",
       "      <td>50.150</td>\n",
       "      <td>51.803</td>\n",
       "      <td>47.521</td>\n",
       "      <td>49.587</td>\n",
       "      <td>51.766</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>51.803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  Player PC Variables  Betting Statistics Variables  \\\n",
       "0                     KNN               41.134                        45.304   \n",
       "1                     LDA               50.075                        51.503   \n",
       "2                     QDA               42.637                        33.959   \n",
       "3             Naive Bayes               46.920                        41.998   \n",
       "4     Logistic regression               50.225                        51.728   \n",
       "5           Decesion tree               38.467                        42.299   \n",
       "6           Random Forest               49.775                        48.009   \n",
       "7                AdaBoost               49.624                        51.653   \n",
       "8   Polynomial kernel SVM               50.413                        51.014   \n",
       "9       Radial kernel SVM               50.864                        51.427   \n",
       "10                    GBM               50.150                        51.803   \n",
       "\n",
       "    Team attribute Variables  \\\n",
       "0                     38.881   \n",
       "1                     45.680   \n",
       "2                     45.605   \n",
       "3                     46.319   \n",
       "4                     45.642   \n",
       "5                     37.716   \n",
       "6                     44.591   \n",
       "7                     47.333   \n",
       "8                     45.192   \n",
       "9                     45.004   \n",
       "10                    47.521   \n",
       "\n",
       "    Team's goal and win percentage rolling Variables  All Variables  \\\n",
       "0                                             40.909         41.773   \n",
       "1                                             49.850         50.789   \n",
       "2                                             45.417         43.050   \n",
       "3                                             47.220         46.243   \n",
       "4                                             50.301         51.390   \n",
       "5                                             40.947         40.759   \n",
       "6                                             48.573         50.488   \n",
       "7                                             50.301         51.277   \n",
       "8                                             50.000         50.826   \n",
       "9                                             49.699         51.165   \n",
       "10                                            49.587         51.766   \n",
       "\n",
       "    Baseline accuracy  All Variables std  \n",
       "0            0.443651             43.576  \n",
       "1            0.443651             50.789  \n",
       "2            0.443651             44.591  \n",
       "3            0.443651             46.243  \n",
       "4            0.443651             51.728  \n",
       "5            0.443651             40.721  \n",
       "6            0.443651             50.413  \n",
       "7            0.443651             51.277  \n",
       "8            0.443651             48.911  \n",
       "9            0.443651             51.127  \n",
       "10           0.443651             51.803  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_all_train, y_all_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_all_test)\n",
    "    y_pred_dict[name] = y_pred\n",
    "    \n",
    "    accuracy = np.mean(y_pred == y_all_test)\n",
    "    \n",
    "    result_accuracy.loc[result_accuracy.model_name == name, \"All Variables\"] = round(accuracy * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = result_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy = np.mean(\"home_win\" == y_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4436513899323817"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[\"Baseline accuracy\"] = baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model_name', 'Player PC Variables', 'Betting Statistics Variables',\n",
       "       'Team attribute Variables',\n",
       "       'Team's goal and win percentage rolling Variables', 'All Variables',\n",
       "       'Baseline accuracy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     41.134\n",
       "1     50.075\n",
       "2     42.637\n",
       "3     46.920\n",
       "4     50.225\n",
       "5     38.467\n",
       "6     49.775\n",
       "7     49.624\n",
       "8     50.413\n",
       "9     50.864\n",
       "10    50.150\n",
       "Name: Player PC Variables, dtype: float64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[[\"Player PC Variables\"]].iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp[['model_name', \"Baseline accuracy\", 'Player PC Variables', 'Betting Statistics Variables',\n",
    "       'Team attribute Variables',\n",
    "       \"Team's goal and win percentage rolling Variables\", \"All Variables\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>Baseline accuracy</th>\n",
       "      <th>Player PC Variables</th>\n",
       "      <th>Betting Statistics Variables</th>\n",
       "      <th>Team attribute Variables</th>\n",
       "      <th>Team's goal and win percentage rolling Variables</th>\n",
       "      <th>All Variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>41.134</td>\n",
       "      <td>45.304</td>\n",
       "      <td>38.881</td>\n",
       "      <td>40.909</td>\n",
       "      <td>41.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.075</td>\n",
       "      <td>51.503</td>\n",
       "      <td>45.680</td>\n",
       "      <td>49.850</td>\n",
       "      <td>50.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QDA</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>42.637</td>\n",
       "      <td>33.959</td>\n",
       "      <td>45.605</td>\n",
       "      <td>45.417</td>\n",
       "      <td>43.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>46.920</td>\n",
       "      <td>41.998</td>\n",
       "      <td>46.319</td>\n",
       "      <td>47.220</td>\n",
       "      <td>46.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.225</td>\n",
       "      <td>51.728</td>\n",
       "      <td>45.642</td>\n",
       "      <td>50.301</td>\n",
       "      <td>51.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decesion tree</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>38.467</td>\n",
       "      <td>42.299</td>\n",
       "      <td>37.716</td>\n",
       "      <td>40.947</td>\n",
       "      <td>40.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>49.775</td>\n",
       "      <td>48.009</td>\n",
       "      <td>44.591</td>\n",
       "      <td>48.573</td>\n",
       "      <td>50.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>49.624</td>\n",
       "      <td>51.653</td>\n",
       "      <td>47.333</td>\n",
       "      <td>50.301</td>\n",
       "      <td>51.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polynomial kernel SVM</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.413</td>\n",
       "      <td>51.014</td>\n",
       "      <td>45.192</td>\n",
       "      <td>50.000</td>\n",
       "      <td>50.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Radial kernel SVM</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.864</td>\n",
       "      <td>51.427</td>\n",
       "      <td>45.004</td>\n",
       "      <td>49.699</td>\n",
       "      <td>51.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GBM</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.150</td>\n",
       "      <td>51.803</td>\n",
       "      <td>47.521</td>\n",
       "      <td>49.587</td>\n",
       "      <td>51.766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  Baseline accuracy  Player PC Variables  \\\n",
       "0                     KNN           0.443651               41.134   \n",
       "1                     LDA           0.443651               50.075   \n",
       "2                     QDA           0.443651               42.637   \n",
       "3             Naive Bayes           0.443651               46.920   \n",
       "4     Logistic regression           0.443651               50.225   \n",
       "5           Decesion tree           0.443651               38.467   \n",
       "6           Random Forest           0.443651               49.775   \n",
       "7                AdaBoost           0.443651               49.624   \n",
       "8   Polynomial kernel SVM           0.443651               50.413   \n",
       "9       Radial kernel SVM           0.443651               50.864   \n",
       "10                    GBM           0.443651               50.150   \n",
       "\n",
       "    Betting Statistics Variables  Team attribute Variables  \\\n",
       "0                         45.304                    38.881   \n",
       "1                         51.503                    45.680   \n",
       "2                         33.959                    45.605   \n",
       "3                         41.998                    46.319   \n",
       "4                         51.728                    45.642   \n",
       "5                         42.299                    37.716   \n",
       "6                         48.009                    44.591   \n",
       "7                         51.653                    47.333   \n",
       "8                         51.014                    45.192   \n",
       "9                         51.427                    45.004   \n",
       "10                        51.803                    47.521   \n",
       "\n",
       "    Team's goal and win percentage rolling Variables  All Variables  \n",
       "0                                             40.909         41.773  \n",
       "1                                             49.850         50.789  \n",
       "2                                             45.417         43.050  \n",
       "3                                             47.220         46.243  \n",
       "4                                             50.301         51.390  \n",
       "5                                             40.947         40.759  \n",
       "6                                             48.573         50.488  \n",
       "7                                             50.301         51.277  \n",
       "8                                             50.000         50.826  \n",
       "9                                             49.699         51.165  \n",
       "10                                            49.587         51.766  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>47.298545</td>\n",
       "      <td>47.336091</td>\n",
       "      <td>44.498545</td>\n",
       "      <td>47.527636</td>\n",
       "      <td>48.138727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1          2          3          4          5          6\n",
       "0  Average  0.443651  47.298545  47.336091  44.498545  47.527636  48.138727"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_avg = pd.DataFrame([\"Average\", 0.443651, 47.298545, 47.336091, 44.498545, 47.527636, 48.138727]).transpose()\n",
    "temp_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_avg.columns = temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>Baseline accuracy</th>\n",
       "      <th>Player PC Variables</th>\n",
       "      <th>Betting Statistics Variables</th>\n",
       "      <th>Team attribute Variables</th>\n",
       "      <th>Team's goal and win percentage rolling Variables</th>\n",
       "      <th>All Variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>47.298545</td>\n",
       "      <td>47.336091</td>\n",
       "      <td>44.498545</td>\n",
       "      <td>47.527636</td>\n",
       "      <td>48.138727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name Baseline accuracy Player PC Variables  \\\n",
       "0    Average          0.443651           47.298545   \n",
       "\n",
       "  Betting Statistics Variables Team attribute Variables  \\\n",
       "0                    47.336091                44.498545   \n",
       "\n",
       "  Team's goal and win percentage rolling Variables All Variables  \n",
       "0                                        47.527636     48.138727  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_merged = pd.concat([temp, temp_avg], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>Baseline accuracy</th>\n",
       "      <th>Player PC Variables</th>\n",
       "      <th>Betting Statistics Variables</th>\n",
       "      <th>Team attribute Variables</th>\n",
       "      <th>Team's goal and win percentage rolling Variables</th>\n",
       "      <th>All Variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>41.134</td>\n",
       "      <td>45.304</td>\n",
       "      <td>38.881</td>\n",
       "      <td>40.909</td>\n",
       "      <td>41.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.075</td>\n",
       "      <td>51.503</td>\n",
       "      <td>45.68</td>\n",
       "      <td>49.85</td>\n",
       "      <td>50.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QDA</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>42.637</td>\n",
       "      <td>33.959</td>\n",
       "      <td>45.605</td>\n",
       "      <td>45.417</td>\n",
       "      <td>43.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>46.92</td>\n",
       "      <td>41.998</td>\n",
       "      <td>46.319</td>\n",
       "      <td>47.22</td>\n",
       "      <td>46.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.225</td>\n",
       "      <td>51.728</td>\n",
       "      <td>45.642</td>\n",
       "      <td>50.301</td>\n",
       "      <td>51.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decesion tree</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>38.467</td>\n",
       "      <td>42.299</td>\n",
       "      <td>37.716</td>\n",
       "      <td>40.947</td>\n",
       "      <td>40.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>49.775</td>\n",
       "      <td>48.009</td>\n",
       "      <td>44.591</td>\n",
       "      <td>48.573</td>\n",
       "      <td>50.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>49.624</td>\n",
       "      <td>51.653</td>\n",
       "      <td>47.333</td>\n",
       "      <td>50.301</td>\n",
       "      <td>51.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polynomial kernel SVM</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.413</td>\n",
       "      <td>51.014</td>\n",
       "      <td>45.192</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Radial kernel SVM</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.864</td>\n",
       "      <td>51.427</td>\n",
       "      <td>45.004</td>\n",
       "      <td>49.699</td>\n",
       "      <td>51.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GBM</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.15</td>\n",
       "      <td>51.803</td>\n",
       "      <td>47.521</td>\n",
       "      <td>49.587</td>\n",
       "      <td>51.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>47.298545</td>\n",
       "      <td>47.336091</td>\n",
       "      <td>44.498545</td>\n",
       "      <td>47.527636</td>\n",
       "      <td>48.138727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name Baseline accuracy Player PC Variables  \\\n",
       "0                     KNN          0.443651              41.134   \n",
       "1                     LDA          0.443651              50.075   \n",
       "2                     QDA          0.443651              42.637   \n",
       "3             Naive Bayes          0.443651               46.92   \n",
       "4     Logistic regression          0.443651              50.225   \n",
       "5           Decesion tree          0.443651              38.467   \n",
       "6           Random Forest          0.443651              49.775   \n",
       "7                AdaBoost          0.443651              49.624   \n",
       "8   Polynomial kernel SVM          0.443651              50.413   \n",
       "9       Radial kernel SVM          0.443651              50.864   \n",
       "10                    GBM          0.443651               50.15   \n",
       "0                 Average          0.443651           47.298545   \n",
       "\n",
       "   Betting Statistics Variables Team attribute Variables  \\\n",
       "0                        45.304                   38.881   \n",
       "1                        51.503                    45.68   \n",
       "2                        33.959                   45.605   \n",
       "3                        41.998                   46.319   \n",
       "4                        51.728                   45.642   \n",
       "5                        42.299                   37.716   \n",
       "6                        48.009                   44.591   \n",
       "7                        51.653                   47.333   \n",
       "8                        51.014                   45.192   \n",
       "9                        51.427                   45.004   \n",
       "10                       51.803                   47.521   \n",
       "0                     47.336091                44.498545   \n",
       "\n",
       "   Team's goal and win percentage rolling Variables All Variables  \n",
       "0                                            40.909        41.773  \n",
       "1                                             49.85        50.789  \n",
       "2                                            45.417         43.05  \n",
       "3                                             47.22        46.243  \n",
       "4                                            50.301         51.39  \n",
       "5                                            40.947        40.759  \n",
       "6                                            48.573        50.488  \n",
       "7                                            50.301        51.277  \n",
       "8                                              50.0        50.826  \n",
       "9                                            49.699        51.165  \n",
       "10                                           49.587        51.766  \n",
       "0                                         47.527636     48.138727  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(temp_merged, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>Baseline accuracy</th>\n",
       "      <th>Player PC Variables</th>\n",
       "      <th>Betting Statistics Variables</th>\n",
       "      <th>Team attribute Variables</th>\n",
       "      <th>Team's goal and win percentage rolling Variables</th>\n",
       "      <th>All Variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>41.134</td>\n",
       "      <td>45.304</td>\n",
       "      <td>38.881</td>\n",
       "      <td>40.909</td>\n",
       "      <td>41.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.075</td>\n",
       "      <td>51.503</td>\n",
       "      <td>45.68</td>\n",
       "      <td>49.85</td>\n",
       "      <td>50.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QDA</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>42.637</td>\n",
       "      <td>33.959</td>\n",
       "      <td>45.605</td>\n",
       "      <td>45.417</td>\n",
       "      <td>43.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>46.92</td>\n",
       "      <td>41.998</td>\n",
       "      <td>46.319</td>\n",
       "      <td>47.22</td>\n",
       "      <td>46.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.225</td>\n",
       "      <td>51.728</td>\n",
       "      <td>45.642</td>\n",
       "      <td>50.301</td>\n",
       "      <td>51.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decesion tree</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>38.467</td>\n",
       "      <td>42.299</td>\n",
       "      <td>37.716</td>\n",
       "      <td>40.947</td>\n",
       "      <td>40.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>49.775</td>\n",
       "      <td>48.009</td>\n",
       "      <td>44.591</td>\n",
       "      <td>48.573</td>\n",
       "      <td>50.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>49.624</td>\n",
       "      <td>51.653</td>\n",
       "      <td>47.333</td>\n",
       "      <td>50.301</td>\n",
       "      <td>51.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polynomial kernel SVM</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.413</td>\n",
       "      <td>51.014</td>\n",
       "      <td>45.192</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Radial kernel SVM</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.864</td>\n",
       "      <td>51.427</td>\n",
       "      <td>45.004</td>\n",
       "      <td>49.699</td>\n",
       "      <td>51.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GBM</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>50.15</td>\n",
       "      <td>51.803</td>\n",
       "      <td>47.521</td>\n",
       "      <td>49.587</td>\n",
       "      <td>51.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average</td>\n",
       "      <td>0.443651</td>\n",
       "      <td>47.298545</td>\n",
       "      <td>47.336091</td>\n",
       "      <td>44.498545</td>\n",
       "      <td>47.527636</td>\n",
       "      <td>48.138727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name Baseline accuracy Player PC Variables  \\\n",
       "0                     KNN          0.443651              41.134   \n",
       "1                     LDA          0.443651              50.075   \n",
       "2                     QDA          0.443651              42.637   \n",
       "3             Naive Bayes          0.443651               46.92   \n",
       "4     Logistic regression          0.443651              50.225   \n",
       "5           Decesion tree          0.443651              38.467   \n",
       "6           Random Forest          0.443651              49.775   \n",
       "7                AdaBoost          0.443651              49.624   \n",
       "8   Polynomial kernel SVM          0.443651              50.413   \n",
       "9       Radial kernel SVM          0.443651              50.864   \n",
       "10                    GBM          0.443651               50.15   \n",
       "0                 Average          0.443651           47.298545   \n",
       "\n",
       "   Betting Statistics Variables Team attribute Variables  \\\n",
       "0                        45.304                   38.881   \n",
       "1                        51.503                    45.68   \n",
       "2                        33.959                   45.605   \n",
       "3                        41.998                   46.319   \n",
       "4                        51.728                   45.642   \n",
       "5                        42.299                   37.716   \n",
       "6                        48.009                   44.591   \n",
       "7                        51.653                   47.333   \n",
       "8                        51.014                   45.192   \n",
       "9                        51.427                   45.004   \n",
       "10                       51.803                   47.521   \n",
       "0                     47.336091                44.498545   \n",
       "\n",
       "   Team's goal and win percentage rolling Variables All Variables  \n",
       "0                                            40.909        41.773  \n",
       "1                                             49.85        50.789  \n",
       "2                                            45.417         43.05  \n",
       "3                                             47.22        46.243  \n",
       "4                                            50.301         51.39  \n",
       "5                                            40.947        40.759  \n",
       "6                                            48.573        50.488  \n",
       "7                                            50.301        51.277  \n",
       "8                                              50.0        50.826  \n",
       "9                                            49.699        51.165  \n",
       "10                                           49.587        51.766  \n",
       "0                                         47.527636     48.138727  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    NaN\n",
       "1    NaN\n",
       "2    NaN\n",
       "3    NaN\n",
       "4    NaN\n",
       "5    NaN\n",
       "6    NaN\n",
       "7    NaN\n",
       "8    NaN\n",
       "9    NaN\n",
       "10   NaN\n",
       "0    NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_merged.drop(\"Baseline accuracy\", axis = 1).groupby(\"model_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The baseline accuracy is achieved by predicting the match result just as home team win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11 models and 4 variable set combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "     'penalty' : ['l1', 'l2'],\n",
    "     'C' : np.logspace(-4, 4, 20),\n",
    "}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/youngjun/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid=[{&#x27;C&#x27;: array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
       "       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
       "       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
       "       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
       "       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n",
       "                          &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid=[{&#x27;C&#x27;: array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
       "       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
       "       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
       "       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
       "       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n",
       "                          &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid=[{'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
       "       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
       "       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
       "       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
       "       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n",
       "                          'penalty': ['l1', 'l2']}])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "log_grid = GridSearchCV(model, param_grid, cv = 5, n_jobs = -1)\n",
    "log_grid.fit(X_all_train_std, y_all_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.0001, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "print(log_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.0001)\n",
      "0.5297437794407899\n"
     ]
    }
   ],
   "source": [
    "print(log_grid.best_estimator_)\n",
    "print(log_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_grid.predict(X_all_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4861006761833208"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred != y_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.0001)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = LogisticRegression(penalty = log_grid.best_params_[\"penalty\"], C = log_grid.best_params_[\"C\"])\n",
    "final_model.fit(X_all_train_std, y_all_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = pd.DataFrame({\"column_name\": X_all_train.columns, \"coef\": final_model.coef_[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>home_player_1_pc_1</td>\n",
       "      <td>0.004586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>home_player_1_pc_2</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>home_player_1_pc_3</td>\n",
       "      <td>-0.007375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>home_player_1_pc_4</td>\n",
       "      <td>0.005856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>home_player_1_pc_5</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>away_team_away_lose_percentage_last_30_matches</td>\n",
       "      <td>-0.006523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>away_team_away_win_percentage_last_60_matches</td>\n",
       "      <td>0.008797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>away_team_away_lose_percentage_last_60_matches</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>away_team_away_win_percentage_last_90_matches</td>\n",
       "      <td>0.005519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>away_team_away_lose_percentage_last_90_matches</td>\n",
       "      <td>0.000852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        column_name      coef\n",
       "0                                home_player_1_pc_1  0.004586\n",
       "1                                home_player_1_pc_2  0.000164\n",
       "2                                home_player_1_pc_3 -0.007375\n",
       "3                                home_player_1_pc_4  0.005856\n",
       "4                                home_player_1_pc_5  0.000052\n",
       "..                                              ...       ...\n",
       "197  away_team_away_lose_percentage_last_30_matches -0.006523\n",
       "198   away_team_away_win_percentage_last_60_matches  0.008797\n",
       "199  away_team_away_lose_percentage_last_60_matches  0.000493\n",
       "200   away_team_away_win_percentage_last_90_matches  0.005519\n",
       "201  away_team_away_lose_percentage_last_90_matches  0.000852\n",
       "\n",
       "[202 rows x 2 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff[\"coef_abs\"] = np.abs(coeff.coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>coef</th>\n",
       "      <th>coef_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>home_player_1_pc_1</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>0.004586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>home_player_1_pc_2</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>home_player_1_pc_3</td>\n",
       "      <td>-0.007375</td>\n",
       "      <td>0.007375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>home_player_1_pc_4</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>0.005856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>home_player_1_pc_5</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>away_team_away_lose_percentage_last_30_matches</td>\n",
       "      <td>-0.006523</td>\n",
       "      <td>0.006523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>away_team_away_win_percentage_last_60_matches</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>0.008797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>away_team_away_lose_percentage_last_60_matches</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>away_team_away_win_percentage_last_90_matches</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>0.005519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>away_team_away_lose_percentage_last_90_matches</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.000852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        column_name      coef  coef_abs\n",
       "0                                home_player_1_pc_1  0.004586  0.004586\n",
       "1                                home_player_1_pc_2  0.000164  0.000164\n",
       "2                                home_player_1_pc_3 -0.007375  0.007375\n",
       "3                                home_player_1_pc_4  0.005856  0.005856\n",
       "4                                home_player_1_pc_5  0.000052  0.000052\n",
       "..                                              ...       ...       ...\n",
       "197  away_team_away_lose_percentage_last_30_matches -0.006523  0.006523\n",
       "198   away_team_away_win_percentage_last_60_matches  0.008797  0.008797\n",
       "199  away_team_away_lose_percentage_last_60_matches  0.000493  0.000493\n",
       "200   away_team_away_win_percentage_last_90_matches  0.005519  0.005519\n",
       "201  away_team_away_lose_percentage_last_90_matches  0.000852  0.000852\n",
       "\n",
       "[202 rows x 3 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>coef</th>\n",
       "      <th>coef_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>home_player_10_pc_1</td>\n",
       "      <td>0.012605</td>\n",
       "      <td>0.012605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>home_player_6_pc_3</td>\n",
       "      <td>-0.012578</td>\n",
       "      <td>0.012578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>home_player_7_pc_1</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>0.012565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>away_player_5_pc_1</td>\n",
       "      <td>-0.012248</td>\n",
       "      <td>0.012248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>away_player_5_pc_3</td>\n",
       "      <td>0.012142</td>\n",
       "      <td>0.012142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>home_player_4_pc_1</td>\n",
       "      <td>0.011918</td>\n",
       "      <td>0.011918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>away_player_9_pc_3</td>\n",
       "      <td>0.011870</td>\n",
       "      <td>0.011870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>away_player_4_pc_1</td>\n",
       "      <td>-0.011756</td>\n",
       "      <td>0.011756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>away_player_8_pc_4</td>\n",
       "      <td>-0.011704</td>\n",
       "      <td>0.011704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>away_player_7_pc_3</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>0.011635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>home_player_7_pc_3</td>\n",
       "      <td>-0.011564</td>\n",
       "      <td>0.011564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>home_player_2_pc_1</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>0.011445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>away_player_7_pc_1</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>0.011415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>away_player_3_pc_3</td>\n",
       "      <td>0.011358</td>\n",
       "      <td>0.011358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>away_player_11_pc_1</td>\n",
       "      <td>-0.011351</td>\n",
       "      <td>0.011351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>home_player_5_pc_1</td>\n",
       "      <td>0.011134</td>\n",
       "      <td>0.011134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>away_player_10_pc_2</td>\n",
       "      <td>-0.011003</td>\n",
       "      <td>0.011003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>home_player_8_pc_3</td>\n",
       "      <td>-0.010871</td>\n",
       "      <td>0.010871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>away_player_9_pc_1</td>\n",
       "      <td>-0.010648</td>\n",
       "      <td>0.010648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>away_player_2_pc_3</td>\n",
       "      <td>0.010564</td>\n",
       "      <td>0.010564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             column_name      coef  coef_abs\n",
       "45   home_player_10_pc_1  0.012605  0.012605\n",
       "27    home_player_6_pc_3 -0.012578  0.012578\n",
       "30    home_player_7_pc_1  0.012565  0.012565\n",
       "75    away_player_5_pc_1 -0.012248  0.012248\n",
       "77    away_player_5_pc_3  0.012142  0.012142\n",
       "15    home_player_4_pc_1  0.011918  0.011918\n",
       "97    away_player_9_pc_3  0.011870  0.011870\n",
       "70    away_player_4_pc_1 -0.011756  0.011756\n",
       "93    away_player_8_pc_4 -0.011704  0.011704\n",
       "87    away_player_7_pc_3  0.011635  0.011635\n",
       "32    home_player_7_pc_3 -0.011564  0.011564\n",
       "5     home_player_2_pc_1  0.011445  0.011445\n",
       "85    away_player_7_pc_1 -0.011415  0.011415\n",
       "67    away_player_3_pc_3  0.011358  0.011358\n",
       "105  away_player_11_pc_1 -0.011351  0.011351\n",
       "20    home_player_5_pc_1  0.011134  0.011134\n",
       "101  away_player_10_pc_2 -0.011003  0.011003\n",
       "37    home_player_8_pc_3 -0.010871  0.010871\n",
       "95    away_player_9_pc_1 -0.010648  0.010648\n",
       "62    away_player_2_pc_3  0.010564  0.010564"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff.sort_values(\"coef_abs\", ascending = False).iloc[21:, :].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff[\"coef\"] = -coeff.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 4))\n",
    "\n",
    "sns.barplot(data = coeff.sort_values(\"coef_abs\", ascending = False).head(15), y = \"column_name\", x = \"coef\", color = \"skyblue\")\n",
    "plt.xlabel(\"Coefficient\", fontsize = 15)\n",
    "plt.ylabel(\"Variable name\", fontsize = 15)\n",
    "plt.title(\"Largest 15 absolute coefficients\", fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.tick_params(axis='y', labelrotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['away_win', 'draw', 'home_win'], dtype=object)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.58639051e-03,  1.64416180e-04, -7.37470346e-03,  5.85602343e-03,\n",
       "        5.22732511e-05,  1.14445806e-02, -6.81104790e-03, -1.61570075e-02,\n",
       "       -2.74653872e-05,  3.39756651e-03,  9.12438549e-03, -6.72223297e-03,\n",
       "       -1.57005397e-02, -5.95447502e-03, -2.18409990e-04,  1.19175122e-02,\n",
       "       -8.28917377e-03, -1.74837438e-02, -6.87943152e-04,  9.19974767e-04,\n",
       "        1.11343826e-02, -2.08861900e-03, -1.44254208e-02,  6.92561851e-03,\n",
       "        2.21392243e-04,  1.48836420e-02, -1.90580627e-03, -1.25776022e-02,\n",
       "        4.04218693e-03, -1.32652446e-03,  1.25652075e-02,  1.58445648e-03,\n",
       "       -1.15638518e-02,  6.69198033e-03,  1.83026974e-03,  1.31121709e-02,\n",
       "        5.69867605e-03, -1.08711710e-02,  6.16623820e-03, -7.43197305e-03,\n",
       "        1.02578994e-02,  4.36209031e-03, -9.49600062e-03,  6.12060291e-03,\n",
       "        4.08790228e-03,  1.26051847e-02,  5.60359804e-04, -1.30959946e-02,\n",
       "        1.22252778e-03,  1.44173425e-03,  1.49340036e-02, -7.93435861e-04,\n",
       "       -1.44362369e-02, -2.02762977e-03,  3.33018687e-03, -2.68374470e-03,\n",
       "       -7.17632991e-03,  6.85433876e-03, -1.24851507e-04,  7.64873703e-04,\n",
       "       -1.33506001e-02, -2.89243792e-03,  1.05641570e-02, -2.25453115e-03,\n",
       "        2.70781218e-03, -8.87796885e-03,  6.46190595e-03,  1.13579676e-02,\n",
       "       -6.29665578e-03, -1.03208707e-02, -1.17564418e-02,  6.95877180e-03,\n",
       "        1.41773387e-02,  6.10721098e-04, -5.72175368e-03, -1.22481556e-02,\n",
       "       -1.29624459e-03,  1.21424797e-02, -4.31086826e-03, -8.27833937e-03,\n",
       "       -9.07919036e-03, -4.13753346e-03,  9.05908777e-03, -4.75919804e-03,\n",
       "       -4.13953367e-03, -1.14147921e-02, -3.07678120e-03,  1.16351409e-02,\n",
       "       -1.68911038e-03,  1.70280240e-03, -1.27767796e-02, -9.33370174e-03,\n",
       "        7.81116061e-03, -1.17038476e-02, -3.71905829e-03, -1.06484089e-02,\n",
       "       -2.98381785e-03,  1.18704674e-02,  8.94789869e-04, -2.07930587e-03,\n",
       "       -1.27486046e-02, -1.10031020e-02,  9.93960359e-03, -2.23354405e-03,\n",
       "       -4.58961221e-03, -1.13507013e-02, -2.25257455e-03,  9.13716126e-03,\n",
       "       -3.05489871e-03, -1.23649284e-03,  3.51555103e-02,  3.44916411e-02,\n",
       "        3.54068554e-02,  3.51555103e-02, -4.11182396e-03, -4.67114843e-03,\n",
       "       -4.49718983e-03, -4.11182396e-03, -2.41945124e-02, -2.22359038e-02,\n",
       "       -2.39080872e-02, -2.41945124e-02,  4.94998959e-04,  2.95367786e-03,\n",
       "       -2.12688602e-03,  3.29053221e-04, -3.09066428e-03, -1.32970880e-03,\n",
       "        5.73899700e-04, -1.47295729e-03, -1.59371652e-04, -1.34093171e-03,\n",
       "        8.90055016e-04,  1.90311220e-03, -1.10141873e-03,  4.33910508e-03,\n",
       "        4.10221776e-03,  1.65777386e-03, -3.30284586e-04,  5.98050961e-03,\n",
       "       -3.25539349e-03,  7.64487771e-03, -3.26525830e-03,  6.02219402e-03,\n",
       "       -6.31539811e-03,  5.48286719e-03, -9.70645428e-03,  1.46262947e-03,\n",
       "       -8.05218221e-03,  1.54963533e-03, -2.53784501e-03,  5.47318859e-04,\n",
       "       -1.29353891e-03,  2.21672433e-04, -2.81357333e-03, -2.95378722e-03,\n",
       "        3.71934614e-03,  1.50827598e-03,  5.01033384e-03, -1.84215938e-03,\n",
       "        7.16088852e-03, -3.79097841e-03,  5.27914161e-03, -7.96031024e-03,\n",
       "        5.75253613e-03, -5.27982752e-03,  7.95562343e-03,  1.54627085e-03,\n",
       "        5.28360321e-03,  1.50806259e-03,  6.56140591e-04,  3.70115097e-03,\n",
       "       -2.34669860e-03,  5.65612527e-03, -3.14440381e-03,  1.75660545e-03,\n",
       "       -4.34277892e-03,  5.08389953e-03, -8.62817528e-03,  4.58759641e-03,\n",
       "       -8.71295851e-03,  2.51023612e-03, -3.35241153e-03,  3.12557741e-04,\n",
       "       -1.58043842e-03,  1.46476184e-03, -3.36026340e-03,  2.84169013e-03,\n",
       "        7.27636880e-04,  1.52213544e-03,  2.91702653e-03,  9.77697050e-04,\n",
       "        7.21899632e-03, -3.24237062e-03,  9.59051067e-03, -7.36191341e-03,\n",
       "        8.11732618e-03, -6.52281031e-03,  8.79663600e-03,  4.92962272e-04,\n",
       "        5.51899751e-03,  8.51772456e-04])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "initialization of _internal failed without raising an exception",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[249], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshap\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/shap/__init__.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mif\u001b[39;00m (sys\u001b[39m.\u001b[39mversion_info \u001b[39m<\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m0\u001b[39m)):\n\u001b[1;32m     10\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAs of version 0.29.0 shap only supports Python 3 (not 2)!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_explanation\u001b[39;00m \u001b[39mimport\u001b[39;00m Explanation, Cohorts\n\u001b[1;32m     14\u001b[0m \u001b[39m# explainers\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexplainers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_explainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Explainer\n",
      "File \u001b[0;32m~/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/shap/_explanation.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslicer\u001b[39;00m \u001b[39mimport\u001b[39;00m Slicer, Alias, Obj\n\u001b[1;32m     11\u001b[0m \u001b[39m# from ._order import Order\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_general\u001b[39;00m \u001b[39mimport\u001b[39;00m OpChain\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_exceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DimensionError\n\u001b[1;32m     15\u001b[0m \u001b[39m# slicer confuses pylint...\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# pylint: disable=no-member\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/shap/utils/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_clustering\u001b[39;00m \u001b[39mimport\u001b[39;00m hclust_ordering, partition_tree, partition_tree_shuffle, delta_minimization_order, hclust\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_general\u001b[39;00m \u001b[39mimport\u001b[39;00m approximate_interactions, potential_interactions, sample, safe_isinstance, assert_import, record_import_error\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_general\u001b[39;00m \u001b[39mimport\u001b[39;00m shapley_coefficients, convert_name, format_value, ordinal_str, OpChain, suppress_stderr\n",
      "File \u001b[0;32m~/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/shap/utils/_clustering.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspatial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m pdist\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m jit\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/numba/__init__.py:42\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m (cfunc, generated_jit, jit, njit, stencil,\n\u001b[1;32m     39\u001b[0m                                    jit_module)\n\u001b[1;32m     41\u001b[0m \u001b[39m# Re-export vectorize decorators and the thread layer querying function\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m \u001b[39mimport\u001b[39;00m (vectorize, guvectorize, threading_layer,\n\u001b[1;32m     43\u001b[0m                             get_num_threads, set_num_threads,\n\u001b[1;32m     44\u001b[0m                             set_parallel_chunksize, get_parallel_chunksize,\n\u001b[1;32m     45\u001b[0m                             get_thread_id)\n\u001b[1;32m     47\u001b[0m \u001b[39m# Re-export Numpy helpers\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy_support\u001b[39;00m \u001b[39mimport\u001b[39;00m carray, farray, from_dtype\n",
      "File \u001b[0;32m~/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/numba/np/ufunc/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m Vectorize, GUVectorize, vectorize, guvectorize\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_internal\u001b[39;00m \u001b[39mimport\u001b[39;00m PyUFunc_None, PyUFunc_Zero, PyUFunc_One\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m \u001b[39mimport\u001b[39;00m _internal, array_exprs\n",
      "File \u001b[0;32m~/anaconda3/envs/data_analysis_venv/lib/python3.10/site-packages/numba/np/ufunc/decorators.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minspect\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m \u001b[39mimport\u001b[39;00m _internal\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mufunc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m ParallelUFuncBuilder, ParallelGUFuncBuilder\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m \u001b[39mimport\u001b[39;00m DelayedRegistry\n",
      "\u001b[0;31mSystemError\u001b[0m: initialization of _internal failed without raising an exception"
     ]
    }
   ],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(final_model, X_all_train_std, feature_names = X_all_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(X_all_test_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)\n",
    "#, X_test_array, feature_names=vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e02e73bf84267195e0a5d7a47b491df6b046c523c3732dc59015fd9619ea8ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
